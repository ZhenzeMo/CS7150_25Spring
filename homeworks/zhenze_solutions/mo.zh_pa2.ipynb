{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1n3hGSNX7NX"
      },
      "source": [
        "# Programming assignment 2: Convolutional Neural Networks (100 points)\n",
        "\n",
        "## Overview\n",
        "<font size='4'> In this assignment you will practice putting together a Convolution Neural Network (CNN) classification pipeline. So far we have worked with deep fully-connected networks, using them to explore different optimization strategies and network architectures. Fully-connected networks are a good testbed for experimentation because they are computationally efficient, but in practice CNNs work better for image classification.\n",
        "\n",
        "<font size='4'>In the first part , you will implement several layer types that are used in convolutional networks using Numpy. In the second part, you will then implement a custom CNN and ones based on the ResNet using PyTorch. You will also practice hyper parameter tuning to achieve desired accuracy.\n",
        "\n",
        "## Submission format\n",
        "* <font size='4'>`<your_nu_username>_pa2.ipynb`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-bQYoL5X7NY"
      },
      "source": [
        "## setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnjiPjyrX7NZ"
      },
      "outputs": [],
      "source": [
        "# As usual, a bit of setup\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "# for auto-reloading external modules\n",
        "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "def rel_error(x, y):\n",
        "    \"\"\" returns relative error \"\"\"\n",
        "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mdfI_ypX7Na",
        "outputId": "ae9ea2ca-e49c-436b-d161-89301b089ce5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-20 17:08:30--  http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 170498071 (163M) [application/x-gzip]\n",
            "Saving to: ‘cifar-10-python.tar.gz’\n",
            "\n",
            "cifar-10-python.tar 100%[===================>] 162.60M  44.4MB/s    in 4.3s    \n",
            "\n",
            "2025-02-20 17:08:35 (37.7 MB/s) - ‘cifar-10-python.tar.gz’ saved [170498071/170498071]\n",
            "\n",
            "cifar-10-batches-py/\n",
            "cifar-10-batches-py/data_batch_4\n",
            "cifar-10-batches-py/readme.html\n",
            "cifar-10-batches-py/test_batch\n",
            "cifar-10-batches-py/data_batch_3\n",
            "cifar-10-batches-py/batches.meta\n",
            "cifar-10-batches-py/data_batch_2\n",
            "cifar-10-batches-py/data_batch_5\n",
            "cifar-10-batches-py/data_batch_1\n"
          ]
        }
      ],
      "source": [
        "# let's download the data\n",
        "# !mkdir ../datasets\n",
        "# !cd ../datasets\n",
        "\n",
        "# 1 -- Linux\n",
        "# 2 -- MacOS\n",
        "# 3 -- Command Prompt on Windows\n",
        "# 4 -- manually downloading the data\n",
        "choice = 1\n",
        "\n",
        "\n",
        "if choice == 1:\n",
        "    # should work well on Linux and in Powershell on Windows\n",
        "    !wget http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
        "elif choice == 2 or choice ==3:\n",
        "    # if wget is not available for you, try curl\n",
        "    # should work well on MacOS\n",
        "    !curl http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz --output cifar-10-python.tar.gz\n",
        "else:\n",
        "    print('Please manually download the data from http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz and put it under the datasets folder.')\n",
        "!tar -xzvf cifar-10-python.tar.gz\n",
        "\n",
        "if choice==3:\n",
        "    !del cifar-10-python.tar.gz\n",
        "else:\n",
        "    !rm cifar-10-python.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnslsToKX7Na"
      },
      "outputs": [],
      "source": [
        "# helpful functions to process and load the data\n",
        "from six.moves import cPickle as pickle\n",
        "import numpy as np\n",
        "import os\n",
        "from imageio import imread\n",
        "import platform\n",
        "\n",
        "def load_pickle(f):\n",
        "    version = platform.python_version_tuple()\n",
        "    if version[0] == '2':\n",
        "        return  pickle.load(f)\n",
        "    elif version[0] == '3':\n",
        "        return  pickle.load(f, encoding='latin1')\n",
        "    raise ValueError(\"invalid python version: {}\".format(version))\n",
        "\n",
        "def load_CIFAR_batch(filename):\n",
        "  \"\"\" load single batch of cifar \"\"\"\n",
        "  with open(filename, 'rb') as f:\n",
        "    datadict = load_pickle(f)\n",
        "    X = datadict['data']\n",
        "    Y = datadict['labels']\n",
        "    X = X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"float\")\n",
        "    Y = np.array(Y)\n",
        "    return X, Y\n",
        "\n",
        "def load_CIFAR10(ROOT):\n",
        "  \"\"\" load all of cifar \"\"\"\n",
        "  xs = []\n",
        "  ys = []\n",
        "  for b in range(1,6):\n",
        "    f = os.path.join(ROOT, 'data_batch_%d' % (b, ))\n",
        "    X, Y = load_CIFAR_batch(f)\n",
        "    xs.append(X)\n",
        "    ys.append(Y)\n",
        "  Xtr = np.concatenate(xs)\n",
        "  Ytr = np.concatenate(ys)\n",
        "  del X, Y\n",
        "  Xte, Yte = load_CIFAR_batch(os.path.join(ROOT, 'test_batch'))\n",
        "  return Xtr, Ytr, Xte, Yte\n",
        "\n",
        "\n",
        "def get_CIFAR10_data(cifar10_dir, num_training=49000, num_validation=1000, num_test=1000,\n",
        "                     subtract_mean=True):\n",
        "    \"\"\"\n",
        "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
        "    it for classifiers. These are the same steps as we used for the SVM, but\n",
        "    condensed to a single function.\n",
        "    \"\"\"\n",
        "    # Load the raw CIFAR-10 data\n",
        "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
        "\n",
        "    # Subsample the data\n",
        "    mask = list(range(num_training, num_training + num_validation))\n",
        "    X_val = X_train[mask]\n",
        "    y_val = y_train[mask]\n",
        "    mask = list(range(num_training))\n",
        "    X_train = X_train[mask]\n",
        "    y_train = y_train[mask]\n",
        "    mask = list(range(num_test))\n",
        "    X_test = X_test[mask]\n",
        "    y_test = y_test[mask]\n",
        "\n",
        "    # Normalize the data: subtract the mean image\n",
        "    if subtract_mean:\n",
        "      mean_image = np.mean(X_train, axis=0)\n",
        "      X_train -= mean_image\n",
        "      X_val -= mean_image\n",
        "      X_test -= mean_image\n",
        "\n",
        "    # Transpose so that channels come first\n",
        "    X_train = X_train.transpose(0, 3, 1, 2).copy()\n",
        "    X_val = X_val.transpose(0, 3, 1, 2).copy()\n",
        "    X_test = X_test.transpose(0, 3, 1, 2).copy()\n",
        "\n",
        "    # Package data into a dictionary\n",
        "    return {\n",
        "      'X_train': X_train, 'y_train': y_train,\n",
        "      'X_val': X_val, 'y_val': y_val,\n",
        "      'X_test': X_test, 'y_test': y_test,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iw9bYKMefeRK",
        "outputId": "b4028705-8c40-4b72-c14d-907d650bc929",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===\n",
            "For the split train\n",
            "shape: (49000, 3, 32, 32)\n",
            "data value range, min: -4.489820571085577, max: 0.8966644435551998\n",
            "\n",
            "===\n",
            "For the split val\n",
            "shape: (1000, 3, 32, 32)\n",
            "data value range, min: -4.489820571085577, max: 0.8966644435551998\n",
            "\n",
            "===\n",
            "For the split test\n",
            "shape: (1000, 3, 32, 32)\n",
            "data value range, min: -4.489820571085577, max: 0.8966644435551998\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Load the (preprocessed) CIFAR10 data.\n",
        "cifar10_dir = './cifar-10-batches-py'\n",
        "\n",
        "data = get_CIFAR10_data(cifar10_dir, subtract_mean=True)\n",
        "\n",
        "pix_mean = (0.485, 0.456, 0.406)\n",
        "pix_std = (0.229, 0.224, 0.225)\n",
        "\n",
        "for c in range(3):\n",
        "    data['X_train'][:, c] = (data['X_train'][:, c] / 255 - pix_mean[c]) / pix_std[c]\n",
        "    data['X_val'][:, c] = (data['X_val'][:, c] / 255 - pix_mean[c]) / pix_std[c]\n",
        "    data['X_test'][:, c] = (data['X_test'][:, c] / 255 - pix_mean[c]) / pix_std[c]\n",
        "\n",
        "for split in ['train', 'val', 'test']:\n",
        "    print('===\\nFor the split {}'.format(split))\n",
        "    print('shape: {}'.format(data['X_{}'.format(split)].shape))\n",
        "    print('data value range, min: {}, max: {}\\n'.format(data['X_{}'.format(split)].min(), data['X_{}'.format(split)].max()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMLkz8Y2X7Na"
      },
      "source": [
        "## Part 1: Implementing convolution and batch normalization layers using Numpy (25 points)\n",
        "(adapted from the work done by Erik Learned-Miller, which was originally developed by Fei-Fei Li, Andrej Karpathy, and Justin Johnson)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2Cz4RqdX7Na"
      },
      "source": [
        "<font size=\"4\" color=\"red\">**task 1.1: forward pass of a convolution layer with two nested for loops (10 points)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dq9kNQomX7Na"
      },
      "outputs": [],
      "source": [
        "def conv_forward_naive(x, w, b, conv_param):\n",
        "    \"\"\"\n",
        "    A naive implementation of the forward pass for a convolutional layer.\n",
        "\n",
        "    The input consists of N data points, each with C channels, height H and\n",
        "    width W. We convolve each input with F different filters, where each filter\n",
        "    spans all C channels and has height HH and width WW.\n",
        "\n",
        "    Input:\n",
        "    - x: Input data of shape (N, C, H, W)\n",
        "    - w: Filter weights of shape (F, C, HH, WW)\n",
        "    - b: Biases, of shape (F,)\n",
        "    - conv_param: A dictionary with the following keys:\n",
        "      - 'stride': The number of pixels between adjacent receptive fields in the\n",
        "        horizontal and vertical directions.\n",
        "      - 'pad': The number of pixels that will be used to zero-pad the input.\n",
        "\n",
        "\n",
        "    During padding, 'pad' zeros should be placed symmetrically (i.e equally on both sides)\n",
        "    along the height and width axes of the input. Be careful not to modfiy the original\n",
        "    input x directly.\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - out: Output data, of shape (N, F, H', W') where H' and W' are given by\n",
        "      H' = ceil((H + 2 * pad - HH + 1) / stride)\n",
        "      W' = ceil((W + 2 * pad - WW + 1) / stride)\n",
        "    - cache: (x, w, b, conv_param)\n",
        "    \"\"\"\n",
        "    out = None\n",
        "\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the convolutional forward pass.                         #\n",
        "    # Hint: you can use the function np.pad for padding.                      #\n",
        "    ###########################################################################\n",
        "    # raise NotImplementedError\n",
        "    # Get the num of the input and the filter\n",
        "    N, C, H, W = x.shape # number of the input, channel, height, width\n",
        "    F, C, HH, WW = w.shape # number of the kernel, channel, height, width\n",
        "    pad, stride = conv_param['pad'], conv_param['stride']\n",
        "\n",
        "    # calculate the size of the output\n",
        "    H_out = np.ceil((H + 2 * pad - HH + 1) / stride).astype(int)\n",
        "    W_out = np.ceil((W + 2 * pad - WW + 1) / stride).astype(int)\n",
        "    # initialize the output\n",
        "    out = np.zeros((N, F, H_out, W_out))\n",
        "    # pad the input\n",
        "    x_padded = np.pad(x, ((0, 0), (0, 0), (pad, pad), (pad, pad)), mode='constant')\n",
        "    # loop over the input\n",
        "    for n in range(N): # all the input\n",
        "        for f in range(F): # all the kernel\n",
        "            for i in range(H_out): # height\n",
        "                for j in range(W_out): # weight\n",
        "                # kernel position\n",
        "                    h_start = i * stride\n",
        "                    h_end = h_start + HH\n",
        "                    w_start = j * stride\n",
        "                    w_end = w_start + WW\n",
        "\n",
        "                    #apply the kernel\n",
        "                    out[n, f, i, j] = np.sum(x_padded[n, :, h_start:h_end, w_start:w_end] * w[f, :, :, :]) + b[f]\n",
        "\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "\n",
        "    cache = (x, w, b, conv_param)\n",
        "    return out, cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-xIe8azX7Na",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f80dede3-8178-44f8-d3e6-c0d82bf8a7e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing conv_forward_naive\n",
            "difference:  2.2121476417505994e-08\n"
          ]
        }
      ],
      "source": [
        "# check your forward pass implementation\n",
        "x_shape = (2, 3, 4, 4)\n",
        "w_shape = (3, 3, 4, 4)\n",
        "x = np.linspace(-0.1, 0.5, num=np.prod(x_shape)).reshape(x_shape)\n",
        "w = np.linspace(-0.2, 0.3, num=np.prod(w_shape)).reshape(w_shape)\n",
        "b = np.linspace(-0.1, 0.2, num=3)\n",
        "\n",
        "conv_param = {'stride': 2, 'pad': 1}\n",
        "out, _ = conv_forward_naive(x, w, b, conv_param)\n",
        "correct_out = np.array([[[[-0.08759809, -0.10987781],\n",
        "                           [-0.18387192, -0.2109216 ]],\n",
        "                          [[ 0.21027089,  0.21661097],\n",
        "                           [ 0.22847626,  0.23004637]],\n",
        "                          [[ 0.50813986,  0.54309974],\n",
        "                           [ 0.64082444,  0.67101435]]],\n",
        "                         [[[-0.98053589, -1.03143541],\n",
        "                           [-1.19128892, -1.24695841]],\n",
        "                          [[ 0.69108355,  0.66880383],\n",
        "                           [ 0.59480972,  0.56776003]],\n",
        "                          [[ 2.36270298,  2.36904306],\n",
        "                           [ 2.38090835,  2.38247847]]]])\n",
        "\n",
        "# Compare your output to ours; difference should be around e-8\n",
        "print('Testing conv_forward_naive')\n",
        "print('difference: ', rel_error(out, correct_out))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOOLqcs1X7Nb"
      },
      "source": [
        "<font size='4' color='red'>**Task 1.2: forward pass of a (normal) batch norm layer (10 points).**\n",
        "\n",
        "<font size='4'>Batch normalization is a very useful technique for training deep neural networks. As proposed in the original paper [1], batch normalization can also be used for convolutional networks, but we need to tweak it a bit; the modification will be called \"spatial batch normalization.\"\n",
        "\n",
        "<font size='4'>Normally batch-normalization accepts inputs of shape `(N, D)` and produces outputs of shape `(N, D)`, where we normalize across the minibatch dimension `N`.\n",
        "\n",
        "[1] [Sergey Ioffe and Christian Szegedy, \"Batch Normalization: Accelerating Deep Network Training by Reducing\n",
        "Internal Covariate Shift\", ICML 2015.](https://arxiv.org/abs/1502.03167)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XT_sIvjPX7Nb"
      },
      "outputs": [],
      "source": [
        "def batchnorm_forward(x, gamma, beta, bn_param):\n",
        "    \"\"\"\n",
        "    Forward pass for batch normalization.\n",
        "\n",
        "    During training the sample mean and (uncorrected) sample variance are\n",
        "    computed from minibatch statistics and used to normalize the incoming data.\n",
        "    During training we also keep an exponentially decaying running mean of the\n",
        "    mean and variance of each feature, and these averages are used to normalize\n",
        "    data at test-time.\n",
        "\n",
        "    At each timestep we update the running averages for mean and variance using\n",
        "    an exponential decay based on the momentum parameter:\n",
        "\n",
        "    running_mean = momentum * running_mean + (1 - momentum) * sample_mean\n",
        "    running_var = momentum * running_var + (1 - momentum) * sample_var\n",
        "\n",
        "    Note that the batch normalization paper suggests a different test-time\n",
        "    behavior: they compute sample mean and variance for each feature using a\n",
        "    large number of training images rather than using a running average. For\n",
        "    this implementation we have chosen to use running averages instead since\n",
        "    they do not require an additional estimation step; the PyTorch\n",
        "    implementation of batch normalization also uses running averages.\n",
        "\n",
        "    Input:\n",
        "    - x: Data of shape (N, D)\n",
        "    - gamma: Scale parameter of shape (D,)\n",
        "    - beta: Shift paremeter of shape (D,)\n",
        "    - bn_param: Dictionary with the following keys:\n",
        "      - mode: 'train' or 'test'; required\n",
        "      - eps: Constant for numeric stability\n",
        "      - momentum: Constant for running mean / variance.\n",
        "      - running_mean: Array of shape (D,) giving running mean of features\n",
        "      - running_var Array of shape (D,) giving running variance of features\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - out: of shape (N, D)\n",
        "    - cache: A tuple of values needed in the backward pass\n",
        "    \"\"\"\n",
        "    mode = bn_param['mode']\n",
        "    eps = bn_param.get('eps', 1e-5)\n",
        "    momentum = bn_param.get('momentum', 0.9) # update ratio\n",
        "\n",
        "    N, D = x.shape\n",
        "    running_mean = bn_param.get('running_mean', np.zeros(D, dtype=x.dtype))\n",
        "    running_var = bn_param.get('running_var', np.zeros(D, dtype=x.dtype))\n",
        "\n",
        "    out, cache = None, None\n",
        "    if mode == 'train':\n",
        "        #######################################################################\n",
        "        # TODO: Implement the training-time forward pass for batch norm.      #\n",
        "        # Use minibatch statistics to compute the mean and variance, use      #\n",
        "        # these statistics to normalize the incoming data, and scale and      #\n",
        "        # shift the normalized data using gamma and beta. Simply treat the    #\n",
        "        # sample mean and sample variance as constants to simplify the        #\n",
        "        # gradients computation.                                              #\n",
        "        #                                                                     #\n",
        "        # You should store the output in the variable out. Any intermediates  #\n",
        "        # that you need for the backward pass should be stored in the cache   #\n",
        "        # variable.                                                           #\n",
        "        #                                                                     #\n",
        "        # You should also use your computed sample mean and variance together #\n",
        "        # with the momentum variable to update the running mean and running   #\n",
        "        # variance, storing your result in the running_mean and running_var   #\n",
        "        # variables.                                                          #\n",
        "        #                                                                     #\n",
        "        # Note that though you should be keeping track of the running         #\n",
        "        # variance, you should normalize the data based on the standard       #\n",
        "        # deviation (square root of variance) instead!                        #\n",
        "        # Referencing the original paper (https://arxiv.org/abs/1502.03167)   #\n",
        "        # might prove to be helpful.                                          #\n",
        "        #######################################################################\n",
        "\n",
        "        # compute the mean\n",
        "        mean = np.mean(x, axis=0)\n",
        "        # compute the variance\n",
        "        var = np.var(x, axis=0)\n",
        "        # normalize the data\n",
        "        x_norm = (x - mean) / np.sqrt(var + eps)\n",
        "        # scale and shift\n",
        "        out = gamma * x_norm + beta\n",
        "\n",
        "        #calculate the running mean and variance for infer\n",
        "        running_mean= momentum * running_mean + (1 - momentum) * mean\n",
        "        running_var = momentum * running_var + (1 - momentum) * var\n",
        "\n",
        "        # stroe the cache for the backpropagation\n",
        "        cache = (x, x_norm, mean, var, gamma, beta, eps)\n",
        "\n",
        "        #######################################################################\n",
        "        #                           END OF YOUR CODE                          #\n",
        "        #######################################################################\n",
        "    elif mode == 'test':\n",
        "        #######################################################################\n",
        "        # TODO: Implement the test-time forward pass for batch normalization. #\n",
        "        # Use the running mean and variance to normalize the incoming data,   #\n",
        "        # then scale and shift the normalized data using gamma and beta.      #\n",
        "        # Store the result in the out variable.                               #\n",
        "        #######################################################################\n",
        "        # normalize the data\n",
        "        x_norm = (x - running_mean) / np.sqrt(running_var + eps)\n",
        "        out = gamma * x_norm + beta\n",
        "        cache = None\n",
        "        #######################################################################\n",
        "        #                          END OF YOUR CODE                           #\n",
        "        #######################################################################\n",
        "    else:\n",
        "        raise ValueError('Invalid forward batchnorm mode \"%s\"' % mode)\n",
        "\n",
        "    # Store the updated running means back into bn_param\n",
        "    bn_param['running_mean'] = running_mean\n",
        "    bn_param['running_var'] = running_var\n",
        "\n",
        "    return out, cache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-YNqB7LX7Nb"
      },
      "source": [
        "<font size='4' color='red'>**Task 1.3: forward pass of a spatial batch norm layer (5 points).**\n",
        "\n",
        "<font size='4'>For data coming from convolutional layers, batch normalization needs to accept inputs of shape `(N, C, H, W)` and produce outputs of shape `(N, C, H, W)` where the `N` dimension gives the minibatch size and the `(H, W)` dimensions give the spatial size of the feature map. In specific, we expect the statistics of each feature channel to be relatively consistent both between different imagesand different locations within the same image. Therefore spatial batch normalization computes a mean and variance for each of the `C` feature channels by computing statistics over both the minibatch dimension `N` and the spatial dimensions `H` and `W`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZ0ydd_7X7Nb"
      },
      "outputs": [],
      "source": [
        "def spatial_batchnorm_forward(x, gamma, beta, bn_param):\n",
        "    \"\"\"\n",
        "    Computes the forward pass for spatial batch normalization.\n",
        "\n",
        "    Inputs:\n",
        "    - x: Input data of shape (N, C, H, W)\n",
        "    - gamma: Scale parameter, of shape (C,)\n",
        "    - beta: Shift parameter, of shape (C,)\n",
        "    - bn_param: Dictionary with the following keys:\n",
        "      - mode: 'train' or 'test'; required\n",
        "      - eps: Constant for numeric stability\n",
        "      - momentum: Constant for running mean / variance. momentum=0 means that\n",
        "        old information is discarded completely at every time step, while\n",
        "        momentum=1 means that new information is never incorporated. The\n",
        "        default of momentum=0.9 should work well in most situations.\n",
        "      - running_mean: Array of shape (D,) giving running mean of features\n",
        "      - running_var Array of shape (D,) giving running variance of features\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - out: Output data, of shape (N, C, H, W)\n",
        "    - cache: Values needed for the backward pass\n",
        "    \"\"\"\n",
        "    out, cache = None, None\n",
        "\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the forward pass for spatial batch normalization.       #\n",
        "    #                                                                         #\n",
        "    # HINT: You can implement spatial batch normalization by calling the      #\n",
        "    # vanilla version of batch normalization you implemented above.           #\n",
        "    # Your implementation should be very short; ours is less than five lines. #\n",
        "    ###########################################################################\n",
        "\n",
        "    # extract the params of the input\n",
        "    N, C, H, W = x.shape\n",
        "    x_reshaped = x.transpose(0, 2, 3, 1).reshape(-1, C) # (N*H*W, C)\n",
        "\n",
        "    # use vanilla batch norm\n",
        "    out_reshaped, cache = batchnorm_forward(x_reshaped, gamma, beta, bn_param)\n",
        "    # reshape the output back to (N,C,H,W)\n",
        "    out = out_reshaped.reshape(N, H, W, C).transpose(0, 3, 1, 2)\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "\n",
        "    return out, cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svaOyaKGX7Nb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "011ee344-e149-4f36-c7d7-482b3e35b17f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before spatial batch normalization:\n",
            "  Shape:  (2, 3, 4, 5)\n",
            "  Means:  [9.33463814 8.90909116 9.11056338]\n",
            "  Stds:  [3.61447857 3.19347686 3.5168142 ]\n",
            "After spatial batch normalization:\n",
            "  Shape:  (2, 3, 4, 5)\n",
            "  Means:  [ 6.18949336e-16  5.99520433e-16 -1.22124533e-16]\n",
            "  Stds:  [0.99999962 0.99999951 0.9999996 ]\n",
            "After spatial batch normalization (nontrivial gamma, beta):\n",
            "  Shape:  (2, 3, 4, 5)\n",
            "  Means:  [6. 7. 8.]\n",
            "  Stds:  [2.99999885 3.99999804 4.99999798]\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(231)\n",
        "# Check the training-time forward pass by checking means and variances\n",
        "# of features both before and after spatial batch normalization\n",
        "\n",
        "N, C, H, W = 2, 3, 4, 5\n",
        "x = 4 * np.random.randn(N, C, H, W) + 10\n",
        "\n",
        "print('Before spatial batch normalization:')\n",
        "print('  Shape: ', x.shape)\n",
        "print('  Means: ', x.mean(axis=(0, 2, 3)))\n",
        "print('  Stds: ', x.std(axis=(0, 2, 3)))\n",
        "\n",
        "# Means should be close to zero and stds close to one\n",
        "gamma, beta = np.ones(C), np.zeros(C)\n",
        "bn_param = {'mode': 'train'}\n",
        "out, _ = spatial_batchnorm_forward(x, gamma, beta, bn_param)\n",
        "print('After spatial batch normalization:')\n",
        "print('  Shape: ', out.shape)\n",
        "print('  Means: ', out.mean(axis=(0, 2, 3)))\n",
        "print('  Stds: ', out.std(axis=(0, 2, 3)))\n",
        "\n",
        "# Means should be close to beta and stds close to gamma\n",
        "gamma, beta = np.asarray([3, 4, 5]), np.asarray([6, 7, 8])\n",
        "out, _ = spatial_batchnorm_forward(x, gamma, beta, bn_param)\n",
        "print('After spatial batch normalization (nontrivial gamma, beta):')\n",
        "print('  Shape: ', out.shape)\n",
        "print('  Means: ', out.mean(axis=(0, 2, 3)))\n",
        "print('  Stds: ', out.std(axis=(0, 2, 3)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMVSh7QPX7Nc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "befa8b21-c9b4-46ab-9a70-03400e3f3dc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After spatial batch normalization (test-time):\n",
            "  means:  [-0.08034406  0.07562881  0.05716371  0.04378383]\n",
            "  stds:  [0.96718744 1.0299714  1.02887624 1.00585577]\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(231)\n",
        "# Check the test-time forward pass by running the training-time\n",
        "# forward pass many times to warm up the running averages, and then\n",
        "# checking the means and variances of activations after a test-time\n",
        "# forward pass.\n",
        "N, C, H, W = 10, 4, 11, 12\n",
        "\n",
        "bn_param = {'mode': 'train'}\n",
        "gamma = np.ones(C)\n",
        "beta = np.zeros(C)\n",
        "for t in range(50):\n",
        "  x = 2.3 * np.random.randn(N, C, H, W) + 13\n",
        "  spatial_batchnorm_forward(x, gamma, beta, bn_param)\n",
        "bn_param['mode'] = 'test'\n",
        "x = 2.3 * np.random.randn(N, C, H, W) + 13\n",
        "a_norm, _ = spatial_batchnorm_forward(x, gamma, beta, bn_param)\n",
        "\n",
        "# Means should be close to zero and stds close to one, but will be\n",
        "# noisier than training-time forward passes.\n",
        "print('After spatial batch normalization (test-time):')\n",
        "print('  means: ', a_norm.mean(axis=(0, 2, 3)))\n",
        "print('  stds: ', a_norm.std(axis=(0, 2, 3)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMs1zpyjX7Nd"
      },
      "source": [
        "## Part 2: Implementing CNNs (Convolutional Neural Networks) using PyTorch (75 points)\n",
        "<font size='4'>You may find the documentation of PyTorch useful https://pytorch.org/docs/stable/index.html."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5lX9yRwX7Nd"
      },
      "source": [
        "<font size='4' color='red'>**Task 2.1: Implement a custom CNN (12 points).**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjD5dpkwZZkI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "    \"\"\"\n",
        "    A simple convolutional network with the following architecture:\n",
        "\n",
        "    [conv - bn - relu] x M - global_average_pooling - affine - softmax\n",
        "\n",
        "    \"[conv - bn - relu] x M\" means the \"conv-bn-relu\" block is repeated for\n",
        "    M times, where M is implicitly defined by the convolution layers' parameters.\n",
        "    Whether to use the batch normalization layer (bn) in-between is a design choice.\n",
        "\n",
        "    For each convolution layer, we do downsampling of factor 2 by setting the stride\n",
        "    to be 2. So we can have a large receptive field size.\n",
        "\n",
        "    The network operates on minibatches of data that have shape (N, C, H, W)\n",
        "    consisting of N images, each with height H and width W and with C input\n",
        "    channels.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim=(3, 32, 32), filter_sizes=[7], filter_channels=[32],\n",
        "            num_classes=10, use_batch_norm=True):\n",
        "        \"\"\"\n",
        "        Initialize a new CNN.\n",
        "\n",
        "        Inputs:\n",
        "        - input_dim: Tuple (C, H, W) giving size of input data\n",
        "        - filter_sizes: Width/height of filters to use in the convolutional layer. It is a\n",
        "          list whose length defines the number of convolution layers.\n",
        "        - filter_channels: Number of filters to use in each convolutional layer. It has the\n",
        "          same length as filter_sizes.\n",
        "        - num_classes: Number of output classes\n",
        "        - use_batch_norm: A boolean variable indicating whether to use batch normalization\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        assert len(filter_sizes) == len(filter_channels), \"Inconsistent filter sizes and channels.\"\n",
        "\n",
        "        ############################################################################\n",
        "        # TODO: Define a set of layers according to the user input.                #\n",
        "        #                                                                          #\n",
        "        # IMPORTANT:                                                               #\n",
        "        # 1. For this assignment, you can assume that the padding of the every     #\n",
        "        # convolutional layer are chosen so that **the width and height of the     #\n",
        "        # input are preserved** (without considering the stride). You need to      #\n",
        "        # carefully set the `pad` parameter for the convolution.                   #\n",
        "        #                                                                          #\n",
        "        # 2. For each convolution layer, we use stride of 2 to do downsampling.    #\n",
        "        ############################################################################\n",
        "        C, H, W = input_dim\n",
        "        layers = [] # store the stack of the cnn\n",
        "        in_channels = C # store the channel\n",
        "        # construct the [conv - bn - relu]\n",
        "        for i in range(len(filter_sizes)):\n",
        "          kernel_size = filter_sizes[i]\n",
        "          out_channels = filter_channels[i]\n",
        "          padding = (kernel_size - 1) // 2 # two sides\n",
        "          stride = 2 #downsampling\n",
        "          layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding))\n",
        "\n",
        "          if use_batch_norm:\n",
        "            layers.append(nn.BatchNorm2d(out_channels)) # add BN layer\n",
        "          layers.append(nn.ReLU(inplace=True)) # ReLU activation\n",
        "          in_channels = out_channels\n",
        "\n",
        "        self.conv_layers = nn.Sequential(*layers)\n",
        "\n",
        "        stride = 2\n",
        "        # calculate the output size\n",
        "        H_out = H // stride ** len(filter_sizes)\n",
        "        W_out = W // stride ** len(filter_sizes)\n",
        "\n",
        "        # global average pooling\n",
        "        self.global_average_pooling = nn.AdaptiveAvgPool2d(1) # (N,C,1,1)\n",
        "        # affine fully connected\n",
        "        self.fc = nn.Linear(in_channels, num_classes)\n",
        "        ############################################################################\n",
        "        #                             END OF YOUR CODE                             #\n",
        "        ############################################################################\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = None\n",
        "        feat_before_gap = None\n",
        "\n",
        "        ############################################################################\n",
        "        # TODO: Implement the forward pass for the simple convolutional net,       #\n",
        "        # computing the class scores for x and storing them in the logits          #\n",
        "        # variable. Also, store the feature map right before the global average    #\n",
        "        # pooling (GAP) layer in the feat_before_gap variable for debugging        #\n",
        "        # purpose only.                                                            #\n",
        "        ############################################################################\n",
        "        # pass for the conv\n",
        "        feat_before_gap = self.conv_layers(x)\n",
        "        # pass for the fc\n",
        "        logits = self.fc(self.global_average_pooling(feat_before_gap).squeeze())\n",
        "        ############################################################################\n",
        "        #                             END OF YOUR CODE                             #\n",
        "        ############################################################################\n",
        "\n",
        "        return logits, feat_before_gap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jm-k2zX_cxJd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac6d14ba-c39b-4f7d-8b5f-caa9d41ab381"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ConvNet(\n",
            "  (conv_layers): Sequential(\n",
            "    (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(4, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (8): ReLU(inplace=True)\n",
            "  )\n",
            "  (global_average_pooling): AdaptiveAvgPool2d(output_size=1)\n",
            "  (fc): Linear(in_features=16, out_features=10, bias=True)\n",
            ")\n",
            "torch.Size([4, 16, 4, 4])\n"
          ]
        }
      ],
      "source": [
        "# Sanity check of the model\n",
        "model = ConvNet(filter_sizes=[3, 3, 3], filter_channels=[4, 8, 16])\n",
        "print(model)\n",
        "\n",
        "x = torch.rand((4, 3, 32, 32))\n",
        "logits, feat_before_gap = model(x)\n",
        "assert logits.shape == torch.Size([4, 10]), \"Incorrect shape for the logits\"\n",
        "print(feat_before_gap.shape)\n",
        "assert feat_before_gap.shape == torch.Size([4, 16, 4, 4]), \"Incorrect shape for the feature map before the GAP layer\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIg6XrIf20ao"
      },
      "source": [
        "<font size='4' color='red'>**Task 2.2: Implement a function to test a CNN (6 points).**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NuN23oh6xXs"
      },
      "outputs": [],
      "source": [
        "# Function to test an already trained model\n",
        "def test_model(model, data_loader):\n",
        "    \"\"\"\n",
        "    Compute accuracy of the model.\n",
        "\n",
        "    Inputs:\n",
        "      - model: A CNN implemented in PyTorch\n",
        "      - data_loader: A data loader that will provide batched images and labels\n",
        "    \"\"\"\n",
        "\n",
        "    # set the model in evaluation mode so the batch norm layers will behave correctly\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
        "    with torch.no_grad():\n",
        "        for batch_data in data_loader:\n",
        "            images, labels = batch_data\n",
        "            images = images.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "            predicted = None\n",
        "            ############################################################################\n",
        "            # TODO: Compute the predicted labels of the batched input images and store #\n",
        "            # them in the predicted varaible.                                          #\n",
        "            ############################################################################\n",
        "            logits, _ = model(images) # calculate the logits\n",
        "            predicted = torch.argmax(logits, dim=1) # get the label for the logits\n",
        "            ############################################################################\n",
        "            #                             END OF YOUR CODE                             #\n",
        "            ############################################################################\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    acc = 100 * correct // total\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3R-tg8Vd26XZ"
      },
      "source": [
        "<font size='4' color='red'>**Task 2.3: Implement a function to train and validate a CNN (11 points).**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3lzA5IOsvtO"
      },
      "outputs": [],
      "source": [
        "def train_val_model(model, train_data_loader, val_data_loader, loss_fn, optimizer, lr_scheduler, num_epochs, print_freq=50):\n",
        "    \"\"\"\n",
        "    Training and validating a CNN model using PyTorch.\n",
        "\n",
        "    Inputs:\n",
        "      - model: A CNN implemented in PyTorch\n",
        "      - data_loader: A data loader that will provide batched images and labels\n",
        "      - loss_fn: A loss function (e.g., cross entropy loss)\n",
        "      - lr_scheduler: Learning rate scheduler\n",
        "      - num_epochs: Number of epochs in total\n",
        "      - print_freq: Frequency to print training statistics\n",
        "\n",
        "    Output:\n",
        "      - model: Trained CNN model\n",
        "    \"\"\"\n",
        "\n",
        "    for epoch_i in range(num_epochs):\n",
        "        # set the model in the train mode so the batch norm layers will behave correctly\n",
        "        model.train()\n",
        "\n",
        "        running_loss = 0.0\n",
        "        running_total = 0.0\n",
        "        running_correct = 0.0\n",
        "        for i, batch_data in enumerate(train_data_loader):\n",
        "            # Every data instance is an image + label pair\n",
        "            images, labels = batch_data\n",
        "            images = images.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "            predicted = None\n",
        "            ############################################################################\n",
        "            # TODO: Finish loss computation, gradient backpropagation, weight update,  #\n",
        "            # and computing the predicted labels of the input images and store them in #\n",
        "            # the predicted varaible, which will be used to monitor the training       #\n",
        "            # accuracy.                                                                #\n",
        "            #                                                                          #\n",
        "            # Note: The learning rate is updated after each **epoch**.                 #\n",
        "            ############################################################################\n",
        "            # calculate the logits\n",
        "            logits,_= model(images)\n",
        "            # logits = logits.float()\n",
        "            # calculate the loss\n",
        "            labels = labels.long()\n",
        "            loss = loss_fn(logits, labels)\n",
        "            # do backpropagation\n",
        "            optimizer.zero_grad() # initialize\n",
        "            loss.backward() # backpropagation\n",
        "            optimizer.step()# update the parameter(weights)\n",
        "            # get the label for the logits\n",
        "            predicted = torch.argmax(logits, dim=1)\n",
        "\n",
        "            ############################################################################\n",
        "            #                             END OF YOUR CODE                             #\n",
        "            ############################################################################\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            running_total += labels.size(0)\n",
        "            running_correct += (predicted == labels).sum().item()\n",
        "            if i % print_freq == 0:    # print every certain number of mini-batches\n",
        "                running_loss = running_loss / print_freq\n",
        "                running_acc = running_correct / running_total * 100\n",
        "                last_lr = lr_scheduler.get_last_lr()[0]\n",
        "                print(f'[{epoch_i + 1}/{num_epochs}, {i + 1:5d}/{len(train_data_loader)}] loss: {running_loss:.3f} acc: {running_acc:.3f} lr: {last_lr:.5f}')\n",
        "                running_loss = 0.0\n",
        "                running_total = 0.0\n",
        "                running_correct = 0.0\n",
        "\n",
        "        # adjust the learning rate\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        val_acc = test_model(model, val_data_loader)\n",
        "        print(f'[{epoch_i + 1}/{num_epochs}] val acc: {val_acc:.3f}')\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S17pDVCR3k-Z"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<font size='4' color='red'>**Task 2.4: Implement a function to set up the loss function, optimizer, and learning rate scheduler (8 points).**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Z62RTUR7GFx"
      },
      "outputs": [],
      "source": [
        "def set_up_loss_optimizer_lr_scheduler(model, learning_rate, momentum, lr_step_size, lr_gamma):\n",
        "    \"\"\"\n",
        "    In this programming assignment, we will adopt the most common choice for the optimizer:\n",
        "    SGD + momentum and learning rate scheduler: StepLR. Please refer to https://pytorch.org/docs/stable/optim.html#algorithms\n",
        "    and https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html#torch.optim.lr_scheduler.StepLR for more details.\n",
        "    \"\"\"\n",
        "    loss_fn = None\n",
        "    optimizer = None\n",
        "    lr_scheduler = None\n",
        "\n",
        "    ############################################################################\n",
        "    # TODO: Define the loss function, optimizer (SGD + momentum), and          #\n",
        "    # learning rate scheduler (StepLR).                                        #\n",
        "    #                                                                          #\n",
        "    # Note: We expect you to set up the learning rate in an epoch-based way.   #\n",
        "    # We will run the learning rate scheduler after each epoch.                #\n",
        "    ############################################################################\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
        "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=lr_step_size, gamma=lr_gamma)\n",
        "    ############################################################################\n",
        "    #                             END OF YOUR CODE                             #\n",
        "    ############################################################################\n",
        "\n",
        "    return loss_fn, optimizer, lr_scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SILUG9AvTl7e"
      },
      "outputs": [],
      "source": [
        "# no need to implement anything here\n",
        "def set_up_cifar10_data_loader(images, labels, batch_size, shuffle=True):\n",
        "    dataset = torch.utils.data.TensorDataset(torch.Tensor(images), torch.Tensor(labels))\n",
        "    data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=2)\n",
        "\n",
        "    return data_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWq-UEpN4irT"
      },
      "source": [
        "<font size='4' color='red'>**Task 2.5: Train a good custom CNN (10 points).**\n",
        "\n",
        "<font size='4'>By tweaking different hyper parameters, such as number of convolution layers, number of filters (channels), learning rate, batch size, etc, you should achieve greater than 60% accuracy on the testing set **with 3 epochs using the SGD + momentum optimizer**.\n",
        "    \n",
        "<font size='4' color='red'>**Note: The total number of parameters of your custom CNN should be smaller than 180K.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3aYdXSHezba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "104ee0d9-1a19-4554-b93c-5ab1dabc1ca6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters: 99.210K\n",
            "There are 1532 batches in the training set.\n",
            "There are 32 batches in the validation set.\n",
            "There are 32 batches in the testing set.\n",
            "[1/3,     1/1532] loss: 0.045 acc: 12.500 lr: 0.01000\n",
            "[1/3,    51/1532] loss: 2.066 acc: 24.688 lr: 0.01000\n",
            "[1/3,   101/1532] loss: 1.849 acc: 32.688 lr: 0.01000\n",
            "[1/3,   151/1532] loss: 1.761 acc: 35.875 lr: 0.01000\n",
            "[1/3,   201/1532] loss: 1.684 acc: 37.250 lr: 0.01000\n",
            "[1/3,   251/1532] loss: 1.624 acc: 39.500 lr: 0.01000\n",
            "[1/3,   301/1532] loss: 1.614 acc: 41.812 lr: 0.01000\n",
            "[1/3,   351/1532] loss: 1.531 acc: 44.250 lr: 0.01000\n",
            "[1/3,   401/1532] loss: 1.455 acc: 47.188 lr: 0.01000\n",
            "[1/3,   451/1532] loss: 1.480 acc: 47.688 lr: 0.01000\n",
            "[1/3,   501/1532] loss: 1.472 acc: 47.312 lr: 0.01000\n",
            "[1/3,   551/1532] loss: 1.433 acc: 46.375 lr: 0.01000\n",
            "[1/3,   601/1532] loss: 1.451 acc: 47.750 lr: 0.01000\n",
            "[1/3,   651/1532] loss: 1.390 acc: 49.062 lr: 0.01000\n",
            "[1/3,   701/1532] loss: 1.458 acc: 48.688 lr: 0.01000\n",
            "[1/3,   751/1532] loss: 1.427 acc: 48.125 lr: 0.01000\n",
            "[1/3,   801/1532] loss: 1.386 acc: 49.438 lr: 0.01000\n",
            "[1/3,   851/1532] loss: 1.403 acc: 49.500 lr: 0.01000\n",
            "[1/3,   901/1532] loss: 1.304 acc: 52.250 lr: 0.01000\n",
            "[1/3,   951/1532] loss: 1.334 acc: 52.750 lr: 0.01000\n",
            "[1/3,  1001/1532] loss: 1.299 acc: 55.062 lr: 0.01000\n",
            "[1/3,  1051/1532] loss: 1.331 acc: 51.125 lr: 0.01000\n",
            "[1/3,  1101/1532] loss: 1.280 acc: 54.312 lr: 0.01000\n",
            "[1/3,  1151/1532] loss: 1.255 acc: 54.500 lr: 0.01000\n",
            "[1/3,  1201/1532] loss: 1.310 acc: 54.125 lr: 0.01000\n",
            "[1/3,  1251/1532] loss: 1.301 acc: 55.812 lr: 0.01000\n",
            "[1/3,  1301/1532] loss: 1.279 acc: 54.062 lr: 0.01000\n",
            "[1/3,  1351/1532] loss: 1.290 acc: 52.312 lr: 0.01000\n",
            "[1/3,  1401/1532] loss: 1.258 acc: 54.062 lr: 0.01000\n",
            "[1/3,  1451/1532] loss: 1.233 acc: 56.875 lr: 0.01000\n",
            "[1/3,  1501/1532] loss: 1.238 acc: 56.188 lr: 0.01000\n",
            "[1/3] val acc: 50.000\n",
            "[2/3,     1/1532] loss: 0.023 acc: 50.000 lr: 0.01000\n",
            "[2/3,    51/1532] loss: 1.151 acc: 58.312 lr: 0.01000\n",
            "[2/3,   101/1532] loss: 1.183 acc: 57.812 lr: 0.01000\n",
            "[2/3,   151/1532] loss: 1.180 acc: 58.562 lr: 0.01000\n",
            "[2/3,   201/1532] loss: 1.228 acc: 56.812 lr: 0.01000\n",
            "[2/3,   251/1532] loss: 1.157 acc: 57.875 lr: 0.01000\n",
            "[2/3,   301/1532] loss: 1.166 acc: 57.563 lr: 0.01000\n",
            "[2/3,   351/1532] loss: 1.170 acc: 57.812 lr: 0.01000\n",
            "[2/3,   401/1532] loss: 1.114 acc: 60.500 lr: 0.01000\n",
            "[2/3,   451/1532] loss: 1.162 acc: 58.375 lr: 0.01000\n",
            "[2/3,   501/1532] loss: 1.184 acc: 56.625 lr: 0.01000\n",
            "[2/3,   551/1532] loss: 1.113 acc: 60.062 lr: 0.01000\n",
            "[2/3,   601/1532] loss: 1.103 acc: 59.312 lr: 0.01000\n",
            "[2/3,   651/1532] loss: 1.157 acc: 59.062 lr: 0.01000\n",
            "[2/3,   701/1532] loss: 1.136 acc: 58.562 lr: 0.01000\n",
            "[2/3,   751/1532] loss: 1.143 acc: 59.750 lr: 0.01000\n",
            "[2/3,   801/1532] loss: 1.169 acc: 57.875 lr: 0.01000\n",
            "[2/3,   851/1532] loss: 1.116 acc: 61.500 lr: 0.01000\n",
            "[2/3,   901/1532] loss: 1.059 acc: 62.500 lr: 0.01000\n",
            "[2/3,   951/1532] loss: 1.166 acc: 60.562 lr: 0.01000\n",
            "[2/3,  1001/1532] loss: 1.115 acc: 61.125 lr: 0.01000\n",
            "[2/3,  1051/1532] loss: 1.100 acc: 60.500 lr: 0.01000\n",
            "[2/3,  1101/1532] loss: 1.111 acc: 60.438 lr: 0.01000\n",
            "[2/3,  1151/1532] loss: 1.123 acc: 59.812 lr: 0.01000\n",
            "[2/3,  1201/1532] loss: 1.110 acc: 61.438 lr: 0.01000\n",
            "[2/3,  1251/1532] loss: 1.134 acc: 60.875 lr: 0.01000\n",
            "[2/3,  1301/1532] loss: 1.089 acc: 61.125 lr: 0.01000\n",
            "[2/3,  1351/1532] loss: 1.125 acc: 60.375 lr: 0.01000\n",
            "[2/3,  1401/1532] loss: 1.068 acc: 61.938 lr: 0.01000\n",
            "[2/3,  1451/1532] loss: 1.085 acc: 61.875 lr: 0.01000\n",
            "[2/3,  1501/1532] loss: 1.131 acc: 61.500 lr: 0.01000\n",
            "[2/3] val acc: 61.000\n",
            "[3/3,     1/1532] loss: 0.025 acc: 59.375 lr: 0.00500\n",
            "[3/3,    51/1532] loss: 0.983 acc: 65.812 lr: 0.00500\n",
            "[3/3,   101/1532] loss: 0.968 acc: 66.062 lr: 0.00500\n",
            "[3/3,   151/1532] loss: 0.888 acc: 69.688 lr: 0.00500\n",
            "[3/3,   201/1532] loss: 0.930 acc: 66.938 lr: 0.00500\n",
            "[3/3,   251/1532] loss: 0.949 acc: 67.062 lr: 0.00500\n",
            "[3/3,   301/1532] loss: 0.977 acc: 65.500 lr: 0.00500\n",
            "[3/3,   351/1532] loss: 0.978 acc: 65.750 lr: 0.00500\n",
            "[3/3,   401/1532] loss: 0.945 acc: 66.500 lr: 0.00500\n",
            "[3/3,   451/1532] loss: 0.939 acc: 67.750 lr: 0.00500\n",
            "[3/3,   501/1532] loss: 0.952 acc: 65.188 lr: 0.00500\n",
            "[3/3,   551/1532] loss: 0.946 acc: 66.250 lr: 0.00500\n",
            "[3/3,   601/1532] loss: 0.929 acc: 66.375 lr: 0.00500\n",
            "[3/3,   651/1532] loss: 0.963 acc: 66.812 lr: 0.00500\n",
            "[3/3,   701/1532] loss: 0.949 acc: 65.188 lr: 0.00500\n",
            "[3/3,   751/1532] loss: 0.940 acc: 66.812 lr: 0.00500\n",
            "[3/3,   801/1532] loss: 0.944 acc: 67.812 lr: 0.00500\n",
            "[3/3,   851/1532] loss: 0.951 acc: 65.750 lr: 0.00500\n",
            "[3/3,   901/1532] loss: 0.983 acc: 65.062 lr: 0.00500\n",
            "[3/3,   951/1532] loss: 0.926 acc: 68.000 lr: 0.00500\n",
            "[3/3,  1001/1532] loss: 0.924 acc: 66.375 lr: 0.00500\n",
            "[3/3,  1051/1532] loss: 0.872 acc: 69.312 lr: 0.00500\n",
            "[3/3,  1101/1532] loss: 0.942 acc: 66.500 lr: 0.00500\n",
            "[3/3,  1151/1532] loss: 0.936 acc: 66.188 lr: 0.00500\n",
            "[3/3,  1201/1532] loss: 0.924 acc: 69.125 lr: 0.00500\n",
            "[3/3,  1251/1532] loss: 0.958 acc: 65.188 lr: 0.00500\n",
            "[3/3,  1301/1532] loss: 0.950 acc: 66.250 lr: 0.00500\n",
            "[3/3,  1351/1532] loss: 0.993 acc: 64.438 lr: 0.00500\n",
            "[3/3,  1401/1532] loss: 0.904 acc: 68.500 lr: 0.00500\n",
            "[3/3,  1451/1532] loss: 0.909 acc: 67.750 lr: 0.00500\n",
            "[3/3,  1501/1532] loss: 0.923 acc: 67.812 lr: 0.00500\n",
            "[3/3] val acc: 65.000\n",
            "testing accuracy: 64.000\n"
          ]
        }
      ],
      "source": [
        "# In practice, this is a hyperparameter to tune.\n",
        "# But here we use a fixed number to make the comparisons fair.\n",
        "num_epochs = 3\n",
        "\n",
        "model = None\n",
        "loss_fn = None\n",
        "optimizer = None\n",
        "lr_scheduler = None\n",
        "############################################################################\n",
        "# TODO: Set up and tune the hyper parameters.                              #\n",
        "############################################################################\n",
        "batch_size = 32\n",
        "learning_rate = 0.01\n",
        "momentum = 0.9\n",
        "lr_gamma = 0.5\n",
        "\n",
        "model = ConvNet(filter_sizes=[3, 3, 3, 3], filter_channels=[16, 32, 64, 128])\n",
        "# loss_fn = nn.CrossEntropyLoss()\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
        "# lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=lr_gamma)\n",
        "loss_fn, optimizer, lr_scheduler = set_up_loss_optimizer_lr_scheduler(model, learning_rate, momentum, 2, lr_gamma)\n",
        "############################################################################\n",
        "#                             END OF YOUR CODE                             #\n",
        "############################################################################\n",
        "\n",
        "model = model.cuda()\n",
        "num_params = sum(p.numel() for p in model.parameters())\n",
        "print('Number of parameters: {:.3f}K'.format(num_params / 1000))\n",
        "\n",
        "# set up the data loaders\n",
        "# note the usage of the batch_size hyperparameter here\n",
        "train_loader = set_up_cifar10_data_loader(data['X_train'], data['y_train'], batch_size, shuffle=True)\n",
        "print(\"There are {} batches in the training set.\".format(len(train_loader)))\n",
        "\n",
        "val_loader = set_up_cifar10_data_loader(data['X_val'], data['y_val'], batch_size, shuffle=False)\n",
        "print(\"There are {} batches in the validation set.\".format(len(val_loader)))\n",
        "\n",
        "test_loader = set_up_cifar10_data_loader(data['X_test'], data['y_test'], batch_size, shuffle=False)\n",
        "print(\"There are {} batches in the testing set.\".format(len(test_loader)))\n",
        "\n",
        "model = train_val_model(model, train_loader, val_loader, loss_fn, optimizer, lr_scheduler, num_epochs)\n",
        "test_acc = test_model(model, test_loader)\n",
        "print(f\"testing accuracy: {test_acc:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvq44ocMCa8Y"
      },
      "source": [
        "<font size='4' color='red'>**Task 2.6: Implement a ResNet-like CNN (11 points).**\n",
        "\n",
        "<font size='4'> In practice, we can borrow the existing model design for our task. ResNet (residual network) is a classical design and being used in many places. Let's experiment with it here. Since we are dealing with small images (32x32), regular ResNets are too deep with too much downsampling. We need to chop off a few blocks to reduce the depth and downsampling factor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wf64PXQ9B5Vt"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "from typing import Any, Callable, List, Optional, Type, Union\n",
        "from torchvision.models.resnet import conv1x1, conv3x3, BasicBlock, Bottleneck, ResNet\n",
        "\n",
        "class MyResNet(ResNet):\n",
        "    def __init__(\n",
        "        self,\n",
        "        block: Type[Union[BasicBlock, Bottleneck]],\n",
        "        layers: List[int],\n",
        "        num_classes: int = 1000,\n",
        "        zero_init_residual: bool = False,\n",
        "        groups: int = 1,\n",
        "        width_per_group: int = 64,\n",
        "        replace_stride_with_dilation: Optional[List[bool]] = None,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Here we will design a model architecture MyResNet, inherited from the ResNet model.\n",
        "        First check here https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py about the\n",
        "        implementation of ResNet in PyTorch.\n",
        "        What you need to do in this part is the remove the layer3 and layer4 and also modify the final\n",
        "        fully-connected layer accordingly.\n",
        "        \"\"\"\n",
        "        super().__init__(\n",
        "            block, layers, num_classes, zero_init_residual, groups,\n",
        "            width_per_group, replace_stride_with_dilation, norm_layer\n",
        "        )\n",
        "\n",
        "        ############################################################################\n",
        "        # TODO: Remove the layer3 and layer4 block in the original implementation  #\n",
        "        # of ResNet and modify the fully-connected layer (classifier) accordingly. #\n",
        "\n",
        "        ############################################################################\n",
        "        del self.layer3\n",
        "        del self.layer4\n",
        "        self.fc = nn.Linear(128 * block.expansion, num_classes)\n",
        "        ############################################################################\n",
        "        #                             END OF YOUR CODE                             #\n",
        "        ############################################################################\n",
        "\n",
        "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
        "        logits = None\n",
        "        feat_before_gap = None\n",
        "        ############################################################################\n",
        "        # TODO: Implement the forward pass for the ResNet-like model,              #\n",
        "        # computing the class scores for x and storing them in the logits          #\n",
        "        # variable. Also, store the feature map right before the global average    #\n",
        "        # pooling (GAP) layer in the feat_before_gap variable for debugging        #\n",
        "        # purpose only.                                                            #\n",
        "        ############################################################################\n",
        "\n",
        "        # conv + BN + ReLU\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        # residual blocks\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        # store the feature map\n",
        "        feat_before_gap = x\n",
        "\n",
        "        x = self.avgpool(x)# (N,C,H,W) -> (N,C,1,1)\n",
        "        x = torch.flatten(x, 1) #(N,C)\n",
        "        logits = self.fc(x)\n",
        "        ############################################################################\n",
        "        #                             END OF YOUR CODE                             #\n",
        "        ############################################################################\n",
        "\n",
        "        return logits, feat_before_gap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7HHhTJzi54Yz"
      },
      "outputs": [],
      "source": [
        "# Let's run a sanity check of your model\n",
        "model = MyResNet(BasicBlock, [2, 2, 2, 2], num_classes=10)\n",
        "\n",
        "x = torch.rand((4, 3, 32, 32))\n",
        "logits, feat_before_gap = model(x)\n",
        "assert logits.shape == torch.Size([4, 10]), \"Incorrect shape for the logits\"\n",
        "assert feat_before_gap.shape[2:] == torch.Size([4, 4]), \"Incorrect shape for the feature map before the GAP layer\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyYdJDdy5yAo"
      },
      "source": [
        "<font size='4' color='red'>**Task 2.7: Train a good custom ResNet-like model (6 points).**\n",
        "\n",
        "<font size='4'>Here we use the same batch size used in the tweaking of your custom CNN. We will also simply use (part of) the ResNet18 model. You only need to tune learning rate, momentum, learning rate decay rate here. You should achieve greater than 70% accuracy on the testing set **with 3 epochs using the SGD + momentum optimizer**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kzrnj9N7DD9N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41e374d5-63d6-45d2-a12f-4756d410fb01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters: 684.362K\n",
            "There are 1532 batches in the training set.\n",
            "There are 32 batches in the validation set.\n",
            "There are 32 batches in the testing set.\n",
            "[1/3,     1/1532] loss: 0.047 acc: 15.625 lr: 0.01000\n",
            "[1/3,    51/1532] loss: 2.025 acc: 23.875 lr: 0.01000\n",
            "[1/3,   101/1532] loss: 1.857 acc: 30.938 lr: 0.01000\n",
            "[1/3,   151/1532] loss: 1.719 acc: 37.062 lr: 0.01000\n",
            "[1/3,   201/1532] loss: 1.655 acc: 37.562 lr: 0.01000\n",
            "[1/3,   251/1532] loss: 1.615 acc: 40.750 lr: 0.01000\n",
            "[1/3,   301/1532] loss: 1.539 acc: 44.375 lr: 0.01000\n",
            "[1/3,   351/1532] loss: 1.542 acc: 43.125 lr: 0.01000\n",
            "[1/3,   401/1532] loss: 1.514 acc: 43.750 lr: 0.01000\n",
            "[1/3,   451/1532] loss: 1.484 acc: 45.000 lr: 0.01000\n",
            "[1/3,   501/1532] loss: 1.433 acc: 49.250 lr: 0.01000\n",
            "[1/3,   551/1532] loss: 1.372 acc: 48.312 lr: 0.01000\n",
            "[1/3,   601/1532] loss: 1.399 acc: 46.938 lr: 0.01000\n",
            "[1/3,   651/1532] loss: 1.366 acc: 50.750 lr: 0.01000\n",
            "[1/3,   701/1532] loss: 1.440 acc: 47.312 lr: 0.01000\n",
            "[1/3,   751/1532] loss: 1.376 acc: 49.312 lr: 0.01000\n",
            "[1/3,   801/1532] loss: 1.317 acc: 52.062 lr: 0.01000\n",
            "[1/3,   851/1532] loss: 1.245 acc: 54.812 lr: 0.01000\n",
            "[1/3,   901/1532] loss: 1.269 acc: 54.250 lr: 0.01000\n",
            "[1/3,   951/1532] loss: 1.252 acc: 54.500 lr: 0.01000\n",
            "[1/3,  1001/1532] loss: 1.240 acc: 56.125 lr: 0.01000\n",
            "[1/3,  1051/1532] loss: 1.209 acc: 56.625 lr: 0.01000\n",
            "[1/3,  1101/1532] loss: 1.169 acc: 58.688 lr: 0.01000\n",
            "[1/3,  1151/1532] loss: 1.207 acc: 56.625 lr: 0.01000\n",
            "[1/3,  1201/1532] loss: 1.183 acc: 57.438 lr: 0.01000\n",
            "[1/3,  1251/1532] loss: 1.110 acc: 60.250 lr: 0.01000\n",
            "[1/3,  1301/1532] loss: 1.240 acc: 55.312 lr: 0.01000\n",
            "[1/3,  1351/1532] loss: 1.181 acc: 57.375 lr: 0.01000\n",
            "[1/3,  1401/1532] loss: 1.157 acc: 58.750 lr: 0.01000\n",
            "[1/3,  1451/1532] loss: 1.130 acc: 60.312 lr: 0.01000\n",
            "[1/3,  1501/1532] loss: 1.061 acc: 61.750 lr: 0.01000\n",
            "[1/3] val acc: 58.000\n",
            "[2/3,     1/1532] loss: 0.019 acc: 62.500 lr: 0.01000\n",
            "[2/3,    51/1532] loss: 1.069 acc: 61.812 lr: 0.01000\n",
            "[2/3,   101/1532] loss: 1.050 acc: 62.313 lr: 0.01000\n",
            "[2/3,   151/1532] loss: 1.050 acc: 62.813 lr: 0.01000\n",
            "[2/3,   201/1532] loss: 1.014 acc: 64.062 lr: 0.01000\n",
            "[2/3,   251/1532] loss: 0.998 acc: 63.625 lr: 0.01000\n",
            "[2/3,   301/1532] loss: 1.030 acc: 64.188 lr: 0.01000\n",
            "[2/3,   351/1532] loss: 0.972 acc: 65.375 lr: 0.01000\n",
            "[2/3,   401/1532] loss: 1.042 acc: 63.438 lr: 0.01000\n",
            "[2/3,   451/1532] loss: 1.018 acc: 65.312 lr: 0.01000\n",
            "[2/3,   501/1532] loss: 1.028 acc: 62.313 lr: 0.01000\n",
            "[2/3,   551/1532] loss: 1.039 acc: 63.188 lr: 0.01000\n",
            "[2/3,   601/1532] loss: 1.014 acc: 64.562 lr: 0.01000\n",
            "[2/3,   651/1532] loss: 1.043 acc: 62.875 lr: 0.01000\n",
            "[2/3,   701/1532] loss: 0.969 acc: 66.375 lr: 0.01000\n",
            "[2/3,   751/1532] loss: 1.045 acc: 62.813 lr: 0.01000\n",
            "[2/3,   801/1532] loss: 0.950 acc: 66.750 lr: 0.01000\n",
            "[2/3,   851/1532] loss: 0.951 acc: 66.000 lr: 0.01000\n",
            "[2/3,   901/1532] loss: 0.968 acc: 65.750 lr: 0.01000\n",
            "[2/3,   951/1532] loss: 0.955 acc: 67.062 lr: 0.01000\n",
            "[2/3,  1001/1532] loss: 0.995 acc: 64.688 lr: 0.01000\n",
            "[2/3,  1051/1532] loss: 0.995 acc: 64.562 lr: 0.01000\n",
            "[2/3,  1101/1532] loss: 0.935 acc: 66.938 lr: 0.01000\n",
            "[2/3,  1151/1532] loss: 0.930 acc: 67.938 lr: 0.01000\n",
            "[2/3,  1201/1532] loss: 0.928 acc: 67.062 lr: 0.01000\n",
            "[2/3,  1251/1532] loss: 0.902 acc: 68.875 lr: 0.01000\n",
            "[2/3,  1301/1532] loss: 0.985 acc: 65.500 lr: 0.01000\n",
            "[2/3,  1351/1532] loss: 0.897 acc: 69.938 lr: 0.01000\n",
            "[2/3,  1401/1532] loss: 0.875 acc: 69.250 lr: 0.01000\n",
            "[2/3,  1451/1532] loss: 0.936 acc: 65.625 lr: 0.01000\n",
            "[2/3,  1501/1532] loss: 0.880 acc: 69.312 lr: 0.01000\n",
            "[2/3] val acc: 62.000\n",
            "[3/3,     1/1532] loss: 0.017 acc: 59.375 lr: 0.00500\n",
            "[3/3,    51/1532] loss: 0.788 acc: 73.312 lr: 0.00500\n",
            "[3/3,   101/1532] loss: 0.784 acc: 73.062 lr: 0.00500\n",
            "[3/3,   151/1532] loss: 0.727 acc: 74.625 lr: 0.00500\n",
            "[3/3,   201/1532] loss: 0.692 acc: 74.875 lr: 0.00500\n",
            "[3/3,   251/1532] loss: 0.728 acc: 74.188 lr: 0.00500\n",
            "[3/3,   301/1532] loss: 0.738 acc: 75.188 lr: 0.00500\n",
            "[3/3,   351/1532] loss: 0.687 acc: 74.562 lr: 0.00500\n",
            "[3/3,   401/1532] loss: 0.710 acc: 74.188 lr: 0.00500\n",
            "[3/3,   451/1532] loss: 0.713 acc: 74.188 lr: 0.00500\n",
            "[3/3,   501/1532] loss: 0.763 acc: 73.438 lr: 0.00500\n",
            "[3/3,   551/1532] loss: 0.700 acc: 75.438 lr: 0.00500\n",
            "[3/3,   601/1532] loss: 0.721 acc: 75.875 lr: 0.00500\n",
            "[3/3,   651/1532] loss: 0.705 acc: 75.438 lr: 0.00500\n",
            "[3/3,   701/1532] loss: 0.723 acc: 73.875 lr: 0.00500\n",
            "[3/3,   751/1532] loss: 0.688 acc: 76.125 lr: 0.00500\n",
            "[3/3,   801/1532] loss: 0.745 acc: 72.625 lr: 0.00500\n",
            "[3/3,   851/1532] loss: 0.762 acc: 73.562 lr: 0.00500\n",
            "[3/3,   901/1532] loss: 0.720 acc: 75.250 lr: 0.00500\n",
            "[3/3,   951/1532] loss: 0.740 acc: 74.188 lr: 0.00500\n",
            "[3/3,  1001/1532] loss: 0.731 acc: 74.000 lr: 0.00500\n",
            "[3/3,  1051/1532] loss: 0.690 acc: 76.750 lr: 0.00500\n",
            "[3/3,  1101/1532] loss: 0.750 acc: 75.500 lr: 0.00500\n",
            "[3/3,  1151/1532] loss: 0.705 acc: 74.000 lr: 0.00500\n",
            "[3/3,  1201/1532] loss: 0.694 acc: 75.938 lr: 0.00500\n",
            "[3/3,  1251/1532] loss: 0.713 acc: 74.875 lr: 0.00500\n",
            "[3/3,  1301/1532] loss: 0.684 acc: 77.312 lr: 0.00500\n",
            "[3/3,  1351/1532] loss: 0.729 acc: 74.625 lr: 0.00500\n",
            "[3/3,  1401/1532] loss: 0.729 acc: 74.062 lr: 0.00500\n",
            "[3/3,  1451/1532] loss: 0.690 acc: 75.062 lr: 0.00500\n",
            "[3/3,  1501/1532] loss: 0.738 acc: 74.062 lr: 0.00500\n",
            "[3/3] val acc: 75.000\n",
            "testing accuracy: 72.000\n"
          ]
        }
      ],
      "source": [
        "# In practice, this is a hyperparameter to tune.\n",
        "# But here we use a fixed number to make the comparisons fair.\n",
        "num_epochs = 3\n",
        "\n",
        "model = MyResNet(BasicBlock, [2, 2, 2, 2], num_classes=10)\n",
        "num_params = sum(p.numel() for p in model.parameters())\n",
        "print('Number of parameters: {:.3f}K'.format(num_params / 1000))\n",
        "\n",
        "############################################################################\n",
        "# TODO: Set up and tune the hyper parameters.                              #\n",
        "############################################################################\n",
        "batch_size = 32\n",
        "learning_rate = 0.01\n",
        "momentum = 0.9\n",
        "lr_gamma = 0.5\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=lr_gamma)\n",
        "############################################################################\n",
        "#                             END OF YOUR CODE                             #\n",
        "############################################################################\n",
        "\n",
        "# set up the data loaders\n",
        "# note the usage of the batch_size hyperparameter here\n",
        "train_loader = set_up_cifar10_data_loader(data['X_train'], data['y_train'], batch_size, shuffle=True)\n",
        "print(\"There are {} batches in the training set.\".format(len(train_loader)))\n",
        "\n",
        "val_loader = set_up_cifar10_data_loader(data['X_val'], data['y_val'], batch_size, shuffle=False)\n",
        "print(\"There are {} batches in the validation set.\".format(len(val_loader)))\n",
        "\n",
        "test_loader = set_up_cifar10_data_loader(data['X_test'], data['y_test'], batch_size, shuffle=False)\n",
        "print(\"There are {} batches in the testing set.\".format(len(test_loader)))\n",
        "\n",
        "model = model.cuda()\n",
        "model = train_val_model(model, train_loader, val_loader, loss_fn, optimizer, lr_scheduler, num_epochs)\n",
        "test_acc = test_model(model, test_loader)\n",
        "print(f\"testing accuracy: {test_acc:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9yxslIX6e7l"
      },
      "source": [
        "<font size='4' color='red'>**Task 2.8: Train a the same ResNet-like model but with ImageNet pre-trained weights (transfer learning, 8 points).**\n",
        "\n",
        "<font size='4'>Here we use the same batch size used in the tweaking of your custom CNN. We will also simply use the same ResNet18-like model. You only need to tune learning rate, momentum, learning rate decay rate here. You should achieve greater than 80% accuracy on the testing set **with 3 epochs using the SGD + momentum optimizer**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJtsipToEiJq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83367e37-712e-4490-b649-11c6b2985ee6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters: 684.362K\n",
            "There are 1532 batches in the training set.\n",
            "There are 32 batches in the validation set.\n",
            "There are 32 batches in the testing set.\n",
            "[1/3,     1/1532] loss: 0.046 acc: 12.500 lr: 0.01000\n",
            "[1/3,    51/1532] loss: 2.268 acc: 15.375 lr: 0.01000\n",
            "[1/3,   101/1532] loss: 2.112 acc: 34.875 lr: 0.01000\n",
            "[1/3,   151/1532] loss: 1.807 acc: 41.812 lr: 0.01000\n",
            "[1/3,   201/1532] loss: 1.556 acc: 47.938 lr: 0.01000\n",
            "[1/3,   251/1532] loss: 1.357 acc: 51.625 lr: 0.01000\n",
            "[1/3,   301/1532] loss: 1.246 acc: 54.937 lr: 0.01000\n",
            "[1/3,   351/1532] loss: 1.169 acc: 59.000 lr: 0.01000\n",
            "[1/3,   401/1532] loss: 1.131 acc: 60.125 lr: 0.01000\n",
            "[1/3,   451/1532] loss: 1.058 acc: 63.125 lr: 0.01000\n",
            "[1/3,   501/1532] loss: 1.024 acc: 64.062 lr: 0.01000\n",
            "[1/3,   551/1532] loss: 0.973 acc: 66.188 lr: 0.01000\n",
            "[1/3,   601/1532] loss: 0.966 acc: 65.875 lr: 0.01000\n",
            "[1/3,   651/1532] loss: 0.937 acc: 66.625 lr: 0.01000\n",
            "[1/3,   701/1532] loss: 0.865 acc: 70.438 lr: 0.01000\n",
            "[1/3,   751/1532] loss: 0.916 acc: 68.688 lr: 0.01000\n",
            "[1/3,   801/1532] loss: 0.914 acc: 68.750 lr: 0.01000\n",
            "[1/3,   851/1532] loss: 0.819 acc: 71.375 lr: 0.01000\n",
            "[1/3,   901/1532] loss: 0.848 acc: 71.312 lr: 0.01000\n",
            "[1/3,   951/1532] loss: 0.848 acc: 71.500 lr: 0.01000\n",
            "[1/3,  1001/1532] loss: 0.857 acc: 69.875 lr: 0.01000\n",
            "[1/3,  1051/1532] loss: 0.876 acc: 69.438 lr: 0.01000\n",
            "[1/3,  1101/1532] loss: 0.829 acc: 70.562 lr: 0.01000\n",
            "[1/3,  1151/1532] loss: 0.791 acc: 72.312 lr: 0.01000\n",
            "[1/3,  1201/1532] loss: 0.781 acc: 73.000 lr: 0.01000\n",
            "[1/3,  1251/1532] loss: 0.740 acc: 73.750 lr: 0.01000\n",
            "[1/3,  1301/1532] loss: 0.720 acc: 74.125 lr: 0.01000\n",
            "[1/3,  1351/1532] loss: 0.780 acc: 72.125 lr: 0.01000\n",
            "[1/3,  1401/1532] loss: 0.759 acc: 73.875 lr: 0.01000\n",
            "[1/3,  1451/1532] loss: 0.745 acc: 75.250 lr: 0.01000\n",
            "[1/3,  1501/1532] loss: 0.715 acc: 75.625 lr: 0.01000\n",
            "[1/3] val acc: 70.000\n",
            "[2/3,     1/1532] loss: 0.010 acc: 81.250 lr: 0.01000\n",
            "[2/3,    51/1532] loss: 0.700 acc: 76.500 lr: 0.01000\n",
            "[2/3,   101/1532] loss: 0.680 acc: 75.375 lr: 0.01000\n",
            "[2/3,   151/1532] loss: 0.662 acc: 77.750 lr: 0.01000\n",
            "[2/3,   201/1532] loss: 0.706 acc: 75.562 lr: 0.01000\n",
            "[2/3,   251/1532] loss: 0.644 acc: 77.125 lr: 0.01000\n",
            "[2/3,   301/1532] loss: 0.693 acc: 77.312 lr: 0.01000\n",
            "[2/3,   351/1532] loss: 0.708 acc: 75.750 lr: 0.01000\n",
            "[2/3,   401/1532] loss: 0.682 acc: 76.625 lr: 0.01000\n",
            "[2/3,   451/1532] loss: 0.686 acc: 76.312 lr: 0.01000\n",
            "[2/3,   501/1532] loss: 0.675 acc: 77.250 lr: 0.01000\n",
            "[2/3,   551/1532] loss: 0.641 acc: 77.438 lr: 0.01000\n",
            "[2/3,   601/1532] loss: 0.627 acc: 78.688 lr: 0.01000\n",
            "[2/3,   651/1532] loss: 0.682 acc: 77.062 lr: 0.01000\n",
            "[2/3,   701/1532] loss: 0.648 acc: 77.375 lr: 0.01000\n",
            "[2/3,   751/1532] loss: 0.652 acc: 76.562 lr: 0.01000\n",
            "[2/3,   801/1532] loss: 0.663 acc: 77.938 lr: 0.01000\n",
            "[2/3,   851/1532] loss: 0.660 acc: 78.188 lr: 0.01000\n",
            "[2/3,   901/1532] loss: 0.668 acc: 77.062 lr: 0.01000\n",
            "[2/3,   951/1532] loss: 0.664 acc: 75.812 lr: 0.01000\n",
            "[2/3,  1001/1532] loss: 0.666 acc: 77.125 lr: 0.01000\n",
            "[2/3,  1051/1532] loss: 0.639 acc: 77.250 lr: 0.01000\n",
            "[2/3,  1101/1532] loss: 0.616 acc: 79.438 lr: 0.01000\n",
            "[2/3,  1151/1532] loss: 0.642 acc: 77.375 lr: 0.01000\n",
            "[2/3,  1201/1532] loss: 0.619 acc: 79.125 lr: 0.01000\n",
            "[2/3,  1251/1532] loss: 0.635 acc: 77.812 lr: 0.01000\n",
            "[2/3,  1301/1532] loss: 0.639 acc: 77.375 lr: 0.01000\n",
            "[2/3,  1351/1532] loss: 0.672 acc: 77.500 lr: 0.01000\n",
            "[2/3,  1401/1532] loss: 0.672 acc: 76.562 lr: 0.01000\n",
            "[2/3,  1451/1532] loss: 0.647 acc: 77.812 lr: 0.01000\n",
            "[2/3,  1501/1532] loss: 0.580 acc: 79.062 lr: 0.01000\n",
            "[2/3] val acc: 76.000\n",
            "[3/3,     1/1532] loss: 0.009 acc: 87.500 lr: 0.00500\n",
            "[3/3,    51/1532] loss: 0.457 acc: 84.688 lr: 0.00500\n",
            "[3/3,   101/1532] loss: 0.469 acc: 84.500 lr: 0.00500\n",
            "[3/3,   151/1532] loss: 0.443 acc: 85.188 lr: 0.00500\n",
            "[3/3,   201/1532] loss: 0.461 acc: 84.188 lr: 0.00500\n",
            "[3/3,   251/1532] loss: 0.446 acc: 85.250 lr: 0.00500\n",
            "[3/3,   301/1532] loss: 0.455 acc: 84.188 lr: 0.00500\n",
            "[3/3,   351/1532] loss: 0.434 acc: 85.000 lr: 0.00500\n",
            "[3/3,   401/1532] loss: 0.416 acc: 85.625 lr: 0.00500\n",
            "[3/3,   451/1532] loss: 0.417 acc: 86.312 lr: 0.00500\n",
            "[3/3,   501/1532] loss: 0.425 acc: 85.562 lr: 0.00500\n",
            "[3/3,   551/1532] loss: 0.449 acc: 84.500 lr: 0.00500\n",
            "[3/3,   601/1532] loss: 0.449 acc: 84.562 lr: 0.00500\n",
            "[3/3,   651/1532] loss: 0.452 acc: 84.875 lr: 0.00500\n",
            "[3/3,   701/1532] loss: 0.442 acc: 85.000 lr: 0.00500\n",
            "[3/3,   751/1532] loss: 0.447 acc: 84.250 lr: 0.00500\n",
            "[3/3,   801/1532] loss: 0.480 acc: 83.188 lr: 0.00500\n",
            "[3/3,   851/1532] loss: 0.429 acc: 84.688 lr: 0.00500\n",
            "[3/3,   901/1532] loss: 0.454 acc: 84.625 lr: 0.00500\n",
            "[3/3,   951/1532] loss: 0.415 acc: 85.375 lr: 0.00500\n",
            "[3/3,  1001/1532] loss: 0.428 acc: 85.312 lr: 0.00500\n",
            "[3/3,  1051/1532] loss: 0.423 acc: 84.875 lr: 0.00500\n",
            "[3/3,  1101/1532] loss: 0.419 acc: 85.750 lr: 0.00500\n",
            "[3/3,  1151/1532] loss: 0.405 acc: 86.188 lr: 0.00500\n",
            "[3/3,  1201/1532] loss: 0.430 acc: 84.750 lr: 0.00500\n",
            "[3/3,  1251/1532] loss: 0.438 acc: 84.375 lr: 0.00500\n",
            "[3/3,  1301/1532] loss: 0.481 acc: 83.500 lr: 0.00500\n",
            "[3/3,  1351/1532] loss: 0.477 acc: 83.938 lr: 0.00500\n",
            "[3/3,  1401/1532] loss: 0.459 acc: 83.438 lr: 0.00500\n",
            "[3/3,  1451/1532] loss: 0.440 acc: 85.438 lr: 0.00500\n",
            "[3/3,  1501/1532] loss: 0.454 acc: 83.750 lr: 0.00500\n",
            "[3/3] val acc: 80.000\n",
            "testing accuracy: 82.000\n"
          ]
        }
      ],
      "source": [
        "# Let's experiment with transfer learning by borrowing the weights of a ResNet model pre-trained on ImageNet.\n",
        "import torchvision\n",
        "imagenet_resnet18 = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights)\n",
        "model = MyResNet(BasicBlock, [2, 2, 2, 2], num_classes=10)\n",
        "num_params = sum(p.numel() for p in model.parameters())\n",
        "print('Number of parameters: {:.3f}K'.format(num_params / 1000))\n",
        "\n",
        "############################################################################\n",
        "# TODO: Copy the appropriate weights from imagenet_resnet18 to our custom  #\n",
        "# model, which shares part of the network architecture.                    #\n",
        "############################################################################\n",
        "model.conv1 = imagenet_resnet18.conv1\n",
        "model.bn1 = imagenet_resnet18.bn1\n",
        "model.maxpool = imagenet_resnet18.maxpool\n",
        "model.layer1 = imagenet_resnet18.layer1\n",
        "model.layer2 = imagenet_resnet18.layer2\n",
        "############################################################################\n",
        "#                             END OF YOUR CODE                             #\n",
        "############################################################################\n",
        "\n",
        "############################################################################\n",
        "# TODO: Set up and tune the hyper parameters.                              #\n",
        "############################################################################\n",
        "learning_rate = 0.01\n",
        "momentum = 0.9\n",
        "lr_gamma = 0.5\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=lr_gamma)\n",
        "############################################################################\n",
        "#                             END OF YOUR CODE                             #\n",
        "############################################################################\n",
        "\n",
        "# set up the data loaders\n",
        "# note the usage of the batch_size hyperparameter here\n",
        "train_loader = set_up_cifar10_data_loader(data['X_train'], data['y_train'], batch_size, shuffle=True)\n",
        "print(\"There are {} batches in the training set.\".format(len(train_loader)))\n",
        "\n",
        "val_loader = set_up_cifar10_data_loader(data['X_val'], data['y_val'], batch_size, shuffle=False)\n",
        "print(\"There are {} batches in the validation set.\".format(len(val_loader)))\n",
        "\n",
        "test_loader = set_up_cifar10_data_loader(data['X_test'], data['y_test'], batch_size, shuffle=False)\n",
        "print(\"There are {} batches in the testing set.\".format(len(test_loader)))\n",
        "\n",
        "model = model.cuda()\n",
        "model = train_val_model(model, train_loader, val_loader, loss_fn, optimizer, lr_scheduler, num_epochs)\n",
        "test_acc = test_model(model, test_loader)\n",
        "print(f\"testing accuracy: {test_acc:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KluK8QC3DiGg"
      },
      "source": [
        "<font size='4' color='red'>**Task 2.9: Briefly explain below why you got increasinly better accuracy from Task 2.5 to Task 2.8 (3 points)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tK3NGPoxDxeS"
      },
      "source": [
        "- **From CNN to CNN + Residual Block:**\n",
        "  - Residual blocks provide shortcut connections, allowing gradients to flow more easily.\n",
        "  - Helps the model learn features more effectively, reducing gradient vanishing and explosion.\n",
        "  - Improves training stability and enables deeper networks.\n",
        "\n",
        "- **From CNN to CNN + Pretrained Model on ImageNet:**\n",
        "  - Leverages pretrained weights from a large dataset (ImageNet), which already captures basic image features.\n",
        "  - Speeds up convergence and improves accuracy on the target dataset.\n",
        "  - Enhances the model’s generalization ability by transferring learned representations.\n",
        "\n",
        "Overall, these incremental improvements lead to better performance and accuracy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_NOHlEorJ0J"
      },
      "source": [
        "<font size='4' color='red'> **Part 3: Extra credits (10 points).**\n",
        "\n",
        "<font size='4'> Let's do something fun here. You can do whatever you can use. Earn the full credits by achieving at least 91% accuracy on the testing set with the following restrictions:\n",
        "- Train the model for no more than 5 epochs.\n",
        "- Use a model whose number of parameters is smaller than 2M.\n",
        "- Use a convolutional neural network\n",
        "\n",
        "<font size='4' color='red'> **Note**: If you have to override any function you implemented earlier, write new code below. Do not change the function definition in previous sections so that we can grade your implementation appopriately.\n",
        "\n",
        "<font size='4' color='red'> **No partial credits will be given to this part. In other words, you won't get any credits if your final testing accuracy is lower than 91%.**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare data\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "class CIFAR10Dataset(Dataset):\n",
        "    def __init__(self, images, labels, transform=None):\n",
        "        self.images = images  # shape: (N, 3, 32, 32)\n",
        "        self.labels = labels  # shape: (N,)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.images[idx].transpose(1, 2, 0)  # (3, 32, 32) -> (32, 32, 3)\n",
        "        img = Image.fromarray((img * 255).astype('uint8'))  # to PIL\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        label = self.labels[idx]\n",
        "        return img, label\n",
        "\n",
        "# test and train augmentation\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.247, 0.243, 0.261])\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.247, 0.243, 0.261])\n",
        "])\n",
        "\n",
        "# modify set_up_cifar10_data_loader\n",
        "def set_up_cifar10_data_loader(images, labels, batch_size, transform, shuffle=True):\n",
        "    dataset = CIFAR10Dataset(images, labels, transform=transform)\n",
        "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=2)\n",
        "    return data_loader\n",
        "\n",
        "batch_size = 32\n",
        "# load augmented data\n",
        "train_loader = set_up_cifar10_data_loader(data['X_train'], data['y_train'], batch_size, transform_train, shuffle=True)\n",
        "val_loader = set_up_cifar10_data_loader(data['X_val'], data['y_val'], batch_size, transform_test, shuffle=False)\n",
        "test_loader = set_up_cifar10_data_loader(data['X_test'], data['y_test'], batch_size, transform_test, shuffle=False)\n",
        "\n",
        "print(f\"train batch: {len(train_loader)}, test batch: {len(test_loader)}\")\n"
      ],
      "metadata": {
        "id": "Cc2qx4sa1yWs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f704d2b6-1afe-44a8-b010-b0928902329c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "训练集批次数: 1532, 测试集批次数: 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T3oTqHulU070"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}