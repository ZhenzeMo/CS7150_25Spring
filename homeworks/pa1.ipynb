{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKKSGzaT36Yg"
      },
      "source": [
        "# Programming Assignment 1: Fully-Connected Networks (100 points)\n",
        "(adapted from the work done by Erik Learned-Miller, which was originally developed by Fei-Fei Li, Andrej Karpathy, and Justin Johnson)\n",
        "\n",
        "## Overview\n",
        "<font size='4'>In this assignment, we will implement fully-connected networks using a modular approach. For each layer we will implement a `forward` and a `backward` function. The `forward` function will receive inputs, weights, and other parameters and will return both an output and a `cache` object storing data needed for the backward pass. After implementing a bunch of layers this way, we will be able to easily combine them to build classifiers with different architectures. In addition to implementing fully-connected networks of arbitrary depth, we will also explore different update rules for optimization.\n",
        "\n",
        "## Submission format\n",
        "* <font size='4'>`<your_nu_username>.ipynb` with your implementations and output.\n",
        "    \n",
        "## Note:\n",
        "* <font size='4'>Do not install any additional packages inside the conda environment. The TAs will use the same environment as defined in the config file we provide you, so anything thatâ€™s not in there by default will probably cause your code to break during grading. Failure to follow any of these instructions will lead to point deductions.\n",
        "* <font size='4'>We have some inline questions embedded in the Jupyter notebook files. Do not miss them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f9cPTmc36Yi"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download helper code\n",
        "!wget https://jianghz.me/teaching/neural_networks/fc_utils.zip\n",
        "!unzip fc_utils.zip"
      ],
      "metadata": {
        "id": "2WuE3L_O3-Qy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pl9EtHVH36Yi"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "from utils.data_utils import load_CIFAR10, get_CIFAR10_data\n",
        "from utils.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array\n",
        "from utils.solver import Solver\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "# for auto-reloading extenrnal modules\n",
        "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "def rel_error(x, y):\n",
        "    \"\"\" returns relative error \"\"\"\n",
        "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XE91G8aM36Yj"
      },
      "outputs": [],
      "source": [
        "# let's download the data\n",
        "# 1 -- Linux\n",
        "# 2 -- MacOS\n",
        "# 3 -- Command Prompt on Windows\n",
        "# 4 -- manually downloading the data\n",
        "choice = 2\n",
        "\n",
        "\n",
        "if choice == 1:\n",
        "    # should work well on Linux and in Powershell on Windows\n",
        "    !wget http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
        "elif choice == 2 or choice ==3:\n",
        "    # if wget is not available for you, try curl\n",
        "    # should work well on MacOS\n",
        "    !curl http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz --output cifar-10-python.tar.gz\n",
        "else:\n",
        "    print('Please manually download the data from http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz and put it under the datasets folder.')\n",
        "!tar -xzvf cifar-10-python.tar.gz\n",
        "\n",
        "if choice==3:\n",
        "    !del cifar-10-python.tar.gz\n",
        "else:\n",
        "    !rm cifar-10-python.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCr5frGU36Yj"
      },
      "outputs": [],
      "source": [
        "# Load the raw CIFAR-10 data.\n",
        "cifar10_dir = './cifar-10-batches-py'\n",
        "\n",
        "# Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
        "try:\n",
        "    del X_train, y_train\n",
        "    del X_test, y_test\n",
        "    print('Clear previously loaded data.')\n",
        "except:\n",
        "    pass\n",
        "\n",
        "X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
        "\n",
        "# As a sanity check, we print out the size of the training and test data.\n",
        "print('Training data shape: ', X_train.shape)\n",
        "print('Training labels shape: ', y_train.shape)\n",
        "print('Test data shape: ', X_test.shape)\n",
        "print('Test labels shape: ', y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJF7LfUm36Yj"
      },
      "outputs": [],
      "source": [
        "# Visualize some examples from the dataset.\n",
        "# We show a few examples of training images from each class.\n",
        "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "num_classes = len(classes)\n",
        "samples_per_class = 7\n",
        "for y, cls in enumerate(classes):\n",
        "    idxs = np.flatnonzero(y_train == y)\n",
        "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
        "    for i, idx in enumerate(idxs):\n",
        "        plt_idx = i * num_classes + y + 1\n",
        "        plt.subplot(samples_per_class, num_classes, plt_idx)\n",
        "        plt.imshow(X_train[idx].astype('uint8'))\n",
        "        plt.axis('off')\n",
        "        if i == 0:\n",
        "            plt.title(cls)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBI5YX9W36Yj"
      },
      "outputs": [],
      "source": [
        "# Split the data into train, val, and test sets.\n",
        "# Check the get_CIFAR10_data function for more details\n",
        "data = get_CIFAR10_data(cifar10_dir)\n",
        "for k, v in list(data.items()):\n",
        "    print(('%s: ' % k, v.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6D6kd1636Yk"
      },
      "source": [
        "# Programming assignments start here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yf7E28nL36Yk"
      },
      "source": [
        "### Part 1: Linear and non-linear layers (40 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWsBjOoe36Yk"
      },
      "source": [
        "<font size='4' color='red'>**Task 1.1: Affine layer: foward pass (no for loops are allowed) (7 points).**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0d9mBdLO36Yk"
      },
      "outputs": [],
      "source": [
        "def affine_forward(x, w, b):\n",
        "    \"\"\"\n",
        "    Computes the forward pass for an affine (fully-connected) layer.\n",
        "\n",
        "    The input x has shape (N, d_1, ..., d_k) and contains a minibatch of N\n",
        "    examples, where each example x[i] has shape (d_1, ..., d_k). We will\n",
        "    reshape each input into a vector of dimension D = d_1 * ... * d_k, and\n",
        "    then transform it to an output vector of dimension M.\n",
        "\n",
        "    Inputs:\n",
        "    - x: A numpy array containing input data, of shape (N, d_1, ..., d_k)\n",
        "    - w: A numpy array of weights, of shape (D, M)\n",
        "    - b: A numpy array of biases, of shape (M,)\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - out: output, of shape (N, M)\n",
        "    - cache: (x, w, b)\n",
        "    \"\"\"\n",
        "    out = None\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the affine forward pass. Store the result in out. You   #\n",
        "    # will need to reshape the input into rows.                               #\n",
        "    ###########################################################################\n",
        "\n",
        "    raise NotImplementedError\n",
        "\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    cache = (x, w, b)\n",
        "    return out, cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgHyhD7l36Yk"
      },
      "outputs": [],
      "source": [
        "# Test the affine_forward function\n",
        "\n",
        "num_inputs = 2\n",
        "input_shape = (4, 5, 6)\n",
        "output_dim = 3\n",
        "\n",
        "input_size = num_inputs * np.prod(input_shape)\n",
        "weight_size = output_dim * np.prod(input_shape)\n",
        "\n",
        "x = np.linspace(-0.1, 0.5, num=input_size).reshape(num_inputs, *input_shape)\n",
        "w = np.linspace(-0.2, 0.3, num=weight_size).reshape(np.prod(input_shape), output_dim)\n",
        "b = np.linspace(-0.3, 0.1, num=output_dim)\n",
        "\n",
        "out, _ = affine_forward(x, w, b)\n",
        "correct_out = np.array([[ 1.49834967,  1.70660132,  1.91485297],\n",
        "                        [ 3.25553199,  3.5141327,   3.77273342]])\n",
        "\n",
        "# Compare your output with ours. The error should be around e-9 or less.\n",
        "print('Testing affine_forward function:')\n",
        "print('difference: ', rel_error(out, correct_out))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZuuxCVw36Yk"
      },
      "source": [
        "<font size='4' color='red'>**Task 1.2: Affine layer: backward pass (no for loops are allowed) (7 points).**\n",
        "    \n",
        "Now implement the `affine_backward` function and test your implementation using numeric gradient checking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WiMFkRVg36Yk"
      },
      "outputs": [],
      "source": [
        "def affine_backward(dout, cache):\n",
        "    \"\"\"\n",
        "    Computes the backward pass for an affine layer.\n",
        "\n",
        "    Inputs:\n",
        "    - dout: Upstream derivative, of shape (N, M)\n",
        "    - cache: Tuple of:\n",
        "      - x: Input data, of shape (N, d_1, ... d_k)\n",
        "      - w: Weights, of shape (D, M)\n",
        "      - b: Biases, of shape (M,)\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - dx: Gradient with respect to x, of shape (N, d1, ..., d_k)\n",
        "    - dw: Gradient with respect to w, of shape (D, M)\n",
        "    - db: Gradient with respect to b, of shape (M,)\n",
        "    \"\"\"\n",
        "    x, w, b = cache\n",
        "    dx, dw, db = None, None, None\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the affine backward pass.                               #\n",
        "    ###########################################################################\n",
        "\n",
        "    raise NotImplementedError\n",
        "\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    return dx, dw, db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOlmwfyg36Yl"
      },
      "outputs": [],
      "source": [
        "# Test the affine_backward function\n",
        "np.random.seed(231)\n",
        "x = np.random.randn(10, 2, 3)\n",
        "w = np.random.randn(6, 5)\n",
        "b = np.random.randn(5)\n",
        "dout = np.random.randn(10, 5)\n",
        "\n",
        "dx_num = eval_numerical_gradient_array(lambda x: affine_forward(x, w, b)[0], x, dout)\n",
        "dw_num = eval_numerical_gradient_array(lambda w: affine_forward(x, w, b)[0], w, dout)\n",
        "db_num = eval_numerical_gradient_array(lambda b: affine_forward(x, w, b)[0], b, dout)\n",
        "\n",
        "_, cache = affine_forward(x, w, b)\n",
        "dx, dw, db = affine_backward(dout, cache)\n",
        "\n",
        "# The error should be around e-10 or less\n",
        "print('Testing affine_backward function:')\n",
        "print('dx error: ', rel_error(dx_num, dx))\n",
        "print('dw error: ', rel_error(dw_num, dw))\n",
        "print('db error: ', rel_error(db_num, db))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUMtu_8A36Yl"
      },
      "source": [
        "<font size='4' color='red'>**Task 1.3: ReLU activation: forward pass (no for loops are allowed) (4 points).**\n",
        "    \n",
        "Implement the forward pass for the ReLU activation function in the `relu_forward` function and test your implementation using the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOY_pAWp36Yl"
      },
      "outputs": [],
      "source": [
        "def relu_forward(x):\n",
        "    \"\"\"\n",
        "    Computes the forward pass for a layer of rectified linear units (ReLUs).\n",
        "\n",
        "    Input:\n",
        "    - x: Inputs, of any shape\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - out: Output, of the same shape as x\n",
        "    - cache: x\n",
        "    \"\"\"\n",
        "    out = None\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the ReLU forward pass.                                  #\n",
        "    ###########################################################################\n",
        "\n",
        "    raise NotImplementedError\n",
        "\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    cache = x\n",
        "    return out, cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60u5t2D236Yl"
      },
      "outputs": [],
      "source": [
        "# Test the relu_forward function\n",
        "\n",
        "x = np.linspace(-0.5, 0.5, num=12).reshape(3, 4)\n",
        "\n",
        "out, _ = relu_forward(x)\n",
        "correct_out = np.array([[ 0.,          0.,          0.,          0.,        ],\n",
        "                        [ 0.,          0.,          0.04545455,  0.13636364,],\n",
        "                        [ 0.22727273,  0.31818182,  0.40909091,  0.5,       ]])\n",
        "\n",
        "# Compare your output with ours. The error should be on the order of e-8\n",
        "print('Testing relu_forward function:')\n",
        "print('difference: ', rel_error(out, correct_out))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f52ckVmM36Yl"
      },
      "source": [
        "<font size='4' color='red'>**Task 1.4: ReLU activation: backward pass (no for loops are allowed) (4 points).**\n",
        "    \n",
        "Now implement the backward pass for the ReLU activation function in the `relu_backward` function and test your implementation using numeric gradient checking:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLXXvB7u36Yl"
      },
      "outputs": [],
      "source": [
        "def relu_backward(dout, cache):\n",
        "    \"\"\"\n",
        "    Computes the backward pass for a layer of rectified linear units (ReLUs).\n",
        "\n",
        "    Input:\n",
        "    - dout: Upstream derivatives, of any shape\n",
        "    - cache: Input x, of same shape as dout\n",
        "\n",
        "    Returns:\n",
        "    - dx: Gradient with respect to x\n",
        "    \"\"\"\n",
        "    dx, x = None, cache\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the ReLU backward pass.                                 #\n",
        "    ###########################################################################\n",
        "\n",
        "    raise NotImplementedError\n",
        "\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    return dx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxF1Vynz36Yl"
      },
      "outputs": [],
      "source": [
        "np.random.seed(231)\n",
        "x = np.random.randn(10, 10)\n",
        "dout = np.random.randn(*x.shape)\n",
        "\n",
        "dx_num = eval_numerical_gradient_array(lambda x: relu_forward(x)[0], x, dout)\n",
        "\n",
        "_, cache = relu_forward(x)\n",
        "dx = relu_backward(dout, cache)\n",
        "\n",
        "# The error should be on the order of e-12\n",
        "print('Testing relu_backward function:')\n",
        "print('dx error: ', rel_error(dx_num, dx))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxXVvWh536Yl"
      },
      "source": [
        "<font size='4' color='red'>**Task 1.5: \"Sandwich\" layers (no for loops are allowed) (8 points where 4 for the forward and 4 for the backward).**\n",
        "    \n",
        "<font size='4'>There are some common patterns of layers that are frequently used in neural nets. For example, affine layers are frequently followed by a ReLU nonlinearity.\n",
        "\n",
        "<font size='4'>Implement the `affine_relu_forward` and `affine_relu_backward` functions, and run the following to numerically gradient check the backward pass.\n",
        "    \n",
        "<font size='4'>You need to re-use the functions you have implemented earlier. You need to decide how to use the cached results given by `affine_forward` and `relu_forward`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gtrlvNn36Yl"
      },
      "outputs": [],
      "source": [
        "def affine_relu_forward(x, w, b):\n",
        "    \"\"\"\n",
        "    Convenience layer that perorms an affine transform followed by a ReLU\n",
        "\n",
        "    Inputs:\n",
        "    - x: Input to the affine layer\n",
        "    - w, b: Weights for the affine layer\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - out: Output from the ReLU\n",
        "    - cache: Object to give to the backward pass\n",
        "    \"\"\"\n",
        "\n",
        "    out, cache = None, None\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the Affine-ReLU forward pass.                           #\n",
        "    ###########################################################################\n",
        "\n",
        "    raise NotImplementedError\n",
        "\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "\n",
        "    return out, cache\n",
        "\n",
        "\n",
        "def affine_relu_backward(dout, cache):\n",
        "    \"\"\"\n",
        "    Backward pass for the affine-relu convenience layer\n",
        "\n",
        "    Input:\n",
        "    - dout: Upstream derivatives, of any shape\n",
        "    - cache: cached results from the affine_relu_forward pass\n",
        "\n",
        "    Returns:\n",
        "    - dx: Gradient with respect to x\n",
        "    - dw: Gradient with respect to w\n",
        "    - db: Gradient with respect to b\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    dx, dw, db = None, None, None\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the Affine-ReLU backward pass.                          #\n",
        "    ###########################################################################\n",
        "\n",
        "    raise NotImplementedError\n",
        "\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "\n",
        "    return dx, dw, db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPyHpm7R36Ym"
      },
      "outputs": [],
      "source": [
        "np.random.seed(231)\n",
        "x = np.random.randn(2, 3, 4)\n",
        "w = np.random.randn(12, 10)\n",
        "b = np.random.randn(10)\n",
        "dout = np.random.randn(2, 10)\n",
        "\n",
        "out, cache = affine_relu_forward(x, w, b)\n",
        "dx, dw, db = affine_relu_backward(dout, cache)\n",
        "\n",
        "dx_num = eval_numerical_gradient_array(lambda x: affine_relu_forward(x, w, b)[0], x, dout)\n",
        "dw_num = eval_numerical_gradient_array(lambda w: affine_relu_forward(x, w, b)[0], w, dout)\n",
        "db_num = eval_numerical_gradient_array(lambda b: affine_relu_forward(x, w, b)[0], b, dout)\n",
        "\n",
        "# Relative error should be around e-10 or less\n",
        "print('Testing affine_relu_forward and affine_relu_backward:')\n",
        "print('dx error: ', rel_error(dx_num, dx))\n",
        "print('dw error: ', rel_error(dw_num, dw))\n",
        "print('db error: ', rel_error(db_num, db))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkp7MtVQ36Ym"
      },
      "source": [
        "<font size='4' color='red'>**Task 1.6: Softmax loss layer (no for loops are allowed) (10 points).**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ue30OZrz36Ym"
      },
      "outputs": [],
      "source": [
        "def softmax_loss(x, y):\n",
        "    \"\"\"\n",
        "    Computes the loss and gradient for softmax classification.\n",
        "\n",
        "    Inputs:\n",
        "    - x: Input data, of shape (N, C) where x[i, j] is the score for the jth\n",
        "      class for the ith input.\n",
        "    - y: Vector of labels, of shape (N,) where y[i] is the label for x[i] and\n",
        "      0 <= y[i] < C\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - loss: Scalar giving the loss\n",
        "    - dx: Gradient of the loss with respect to x\n",
        "    \"\"\"\n",
        "\n",
        "    loss, dx = None, None\n",
        "\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the softmax_loss function,                              #\n",
        "    # including the forward and backward passes.                              #\n",
        "    ###########################################################################\n",
        "\n",
        "    raise NotImplementedError\n",
        "\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "\n",
        "    return loss, dx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1NEqR2X36Ym"
      },
      "outputs": [],
      "source": [
        "# Let's check your implementation\n",
        "np.random.seed(5330)\n",
        "num_classes, num_inputs = 10, 50\n",
        "x = 0.001 * np.random.randn(num_inputs, num_classes)\n",
        "y = np.random.randint(num_classes, size=num_inputs)\n",
        "\n",
        "dx_num = eval_numerical_gradient(lambda x: softmax_loss(x, y)[0], x, verbose=False)\n",
        "loss, dx = softmax_loss(x, y)\n",
        "\n",
        "# Test softmax_loss function. Loss should be close to 2.3 and dx error should be around e-8\n",
        "print('\\nTesting softmax_loss:')\n",
        "print('loss: ', loss)\n",
        "print('dx error: ', rel_error(dx_num, dx))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKAbjCJK36Ym"
      },
      "source": [
        "### Part 2: Fully-Connected Neural Networks (60 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKJ1Sjq036Ym"
      },
      "source": [
        "<font size='4' color='red'>**Task 2.1: Two-layer network (no for loops are allowed in your implementation) (15 points)**\n",
        "    \n",
        "<font size='4'>Complete the implementation of the `TwoLayerNet` class. This class will serve as a model for the other networks you will implement in this assignment, so read through it to make sure you understand the API. You can run the cell below to test your implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtotosQ836Ym"
      },
      "outputs": [],
      "source": [
        "class TwoLayerNet(object):\n",
        "    \"\"\"\n",
        "    A two-layer fully-connected neural network with ReLU nonlinearity and\n",
        "    softmax loss that uses a modular layer design. We assume an input dimension\n",
        "    of D, a hidden dimension of H, and perform classification over C classes.\n",
        "\n",
        "    The architecure should be affine - relu - affine - softmax.\n",
        "\n",
        "    Note that this class does not implement gradient descent; instead, it\n",
        "    will interact with a separate Solver object that is responsible for running\n",
        "    optimization.\n",
        "\n",
        "    The learnable parameters of the model are stored in the dictionary\n",
        "    self.params that maps parameter names to numpy arrays.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim=3*32*32, hidden_dim=100, num_classes=10,\n",
        "                 weight_scale=1e-3, reg=0.0):\n",
        "        \"\"\"\n",
        "        Initialize a new network.\n",
        "\n",
        "        Inputs:\n",
        "        - input_dim: An integer giving the size of the input\n",
        "        - hidden_dim: An integer giving the size of the hidden layer\n",
        "        - num_classes: An integer giving the number of classes to classify\n",
        "        - weight_scale: Scalar giving the standard deviation for random\n",
        "          initialization of the weights.\n",
        "        - reg: Scalar giving L2 regularization strength.\n",
        "        \"\"\"\n",
        "        self.params = {}\n",
        "        self.reg = reg\n",
        "\n",
        "        ############################################################################\n",
        "        # TODO: Initialize the weights and biases of the two-layer net. Weights    #\n",
        "        # should be initialized from a Gaussian centered at 0.0 with               #\n",
        "        # standard deviation equal to weight_scale, and biases should be           #\n",
        "        # initialized to zero. All weights and biases should be stored in the      #\n",
        "        # dictionary self.params, with first layer weights                         #\n",
        "        # and biases using the keys 'W1' and 'b1' and second layer                 #\n",
        "        # weights and biases using the keys 'W2' and 'b2'.                         #\n",
        "        ############################################################################\n",
        "\n",
        "        raise NotImplementedError\n",
        "\n",
        "        ############################################################################\n",
        "        #                             END OF YOUR CODE                             #\n",
        "        ############################################################################\n",
        "\n",
        "\n",
        "    def loss(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Compute loss and gradient for a minibatch of data.\n",
        "\n",
        "        Inputs:\n",
        "        - X: Array of input data of shape (N, d_1, ..., d_k)\n",
        "        - y: Array of labels, of shape (N,). y[i] gives the label for X[i].\n",
        "\n",
        "        Returns:\n",
        "        If y is None, then run a test-time forward pass of the model and return:\n",
        "        - scores: Array of shape (N, C) giving classification scores, where\n",
        "          scores[i, c] is the classification score for X[i] and class c.\n",
        "\n",
        "        If y is not None, then run a training-time forward and backward pass and\n",
        "        return a tuple of:\n",
        "        - loss: Scalar value giving the loss\n",
        "        - grads: Dictionary with the same keys as self.params, mapping parameter\n",
        "          names to gradients of the loss with respect to those parameters.\n",
        "        \"\"\"\n",
        "        scores = None\n",
        "        ############################################################################\n",
        "        # TODO: Implement the forward pass for the two-layer net, computing the    #\n",
        "        # class scores for X and storing them in the scores variable.              #\n",
        "        ############################################################################\n",
        "\n",
        "        raise NotImplementedError\n",
        "\n",
        "        ############################################################################\n",
        "        #                             END OF YOUR CODE                             #\n",
        "        ############################################################################\n",
        "\n",
        "        # If y is None then we are in test mode so just return scores\n",
        "        if y is None:\n",
        "            return scores\n",
        "\n",
        "        loss, grads = 0, {}\n",
        "        ############################################################################\n",
        "        # TODO: Implement the backward pass for the two-layer net. Store the loss  #\n",
        "        # in the loss variable and gradients in the grads dictionary. Compute data #\n",
        "        # loss using softmax, and make sure that grads[k] holds the gradients for  #\n",
        "        # self.params[k]. Don't forget to add L2 regularization!                   #\n",
        "        #                                                                          #\n",
        "        # NOTE: To ensure that your implementation matches ours and you pass the   #\n",
        "        # automated tests, make sure that your L2 regularization includes a factor #\n",
        "        # of 0.5 to simplify the expression for the gradient. It's sufficient to   #\n",
        "        # compute the L2 regularization for the weights (W1, W2) only.             #\n",
        "        ############################################################################\n",
        "\n",
        "        raise NotImplementedError\n",
        "\n",
        "        ############################################################################\n",
        "        #                             END OF YOUR CODE                             #\n",
        "        ############################################################################\n",
        "\n",
        "        return loss, grads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oy-Zo6iG36Ym"
      },
      "outputs": [],
      "source": [
        "# Let's check your implementation\n",
        "np.random.seed(5330)\n",
        "N, D, H, C = 3, 5, 50, 7\n",
        "X = np.random.randn(N, D)\n",
        "y = np.random.randint(C, size=N)\n",
        "\n",
        "std = 1e-3\n",
        "model = TwoLayerNet(input_dim=D, hidden_dim=H, num_classes=C, weight_scale=std)\n",
        "\n",
        "print('Testing initialization ... ')\n",
        "W1_std = abs(model.params['W1'].std() - std)\n",
        "b1 = model.params['b1']\n",
        "W2_std = abs(model.params['W2'].std() - std)\n",
        "b2 = model.params['b2']\n",
        "assert W1_std < std / 10, 'First layer weights do not seem right'\n",
        "assert np.all(b1 == 0), 'First layer biases do not seem right'\n",
        "assert W2_std < std / 10, 'Second layer weights do not seem right'\n",
        "assert np.all(b2 == 0), 'Second layer biases do not seem right'\n",
        "\n",
        "print('Testing test-time forward pass ... ')\n",
        "model.params['W1'] = np.linspace(-0.7, 0.3, num=D*H).reshape(D, H)\n",
        "model.params['b1'] = np.linspace(-0.1, 0.9, num=H)\n",
        "model.params['W2'] = np.linspace(-0.3, 0.4, num=H*C).reshape(H, C)\n",
        "model.params['b2'] = np.linspace(-0.9, 0.1, num=C)\n",
        "X = np.linspace(-5.5, 4.5, num=N*D).reshape(D, N).T\n",
        "scores = model.loss(X)\n",
        "correct_scores = np.asarray(\n",
        "  [[11.53165108,  12.2917344,   13.05181771,  13.81190102,  14.57198434, 15.33206765,  16.09215096],\n",
        "   [12.05769098,  12.74614105,  13.43459113,  14.1230412,   14.81149128, 15.49994135,  16.18839143],\n",
        "   [12.58373087,  13.20054771,  13.81736455,  14.43418138,  15.05099822, 15.66781506,  16.2846319 ]])\n",
        "scores_diff = np.abs(scores - correct_scores).sum()\n",
        "assert scores_diff < 1e-6, 'Problem with test-time forward pass'\n",
        "\n",
        "print('Testing training loss (no regularization)')\n",
        "y = np.asarray([0, 5, 1])\n",
        "loss, grads = model.loss(X, y)\n",
        "correct_loss = 3.4702243556\n",
        "assert abs(loss - correct_loss) < 1e-10, 'Problem with training-time loss'\n",
        "\n",
        "model.reg = 1.0\n",
        "loss, grads = model.loss(X, y)\n",
        "correct_loss = 26.5948426952\n",
        "assert abs(loss - correct_loss) < 1e-10, 'Problem with regularization loss'\n",
        "\n",
        "# Errors should be around e-7 or less\n",
        "for reg in [0.0, 0.7]:\n",
        "  print('Running numeric gradient check with reg = ', reg)\n",
        "  model.reg = reg\n",
        "  loss, grads = model.loss(X, y)\n",
        "\n",
        "  for name in sorted(grads):\n",
        "    f = lambda _: model.loss(X, y)[0]\n",
        "    grad_num = eval_numerical_gradient(f, model.params[name], verbose=False)\n",
        "    print('%s relative error: %.2e' % (name, rel_error(grad_num, grads[name])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGUYTB7K36Yn"
      },
      "source": [
        "<font size='4' color='red'>**Task 2.2: Solver (3 points).**\n",
        "    \n",
        "<font size='4'>Following a more modular design, we have split the logic for training models into a separate class.\n",
        "\n",
        "<font size='4'>Open the file `utils/solver.py` and read through it to familiarize yourself with the API. After doing so, use a `Solver` instance to train a `TwoLayerNet` that <font color='red'>achieves at least `50%` accuracy on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RaqDZe5J36Yn"
      },
      "outputs": [],
      "source": [
        "from utils.solver import Solver\n",
        "\n",
        "model = TwoLayerNet()\n",
        "solver = None\n",
        "\n",
        "##############################################################################\n",
        "# TODO: Use a Solver instance to train a TwoLayerNet that achieves at least  #\n",
        "# 50% accuracy on the validation set.                                        #\n",
        "##############################################################################\n",
        "\n",
        "raise NotImplementedError\n",
        "\n",
        "##############################################################################\n",
        "#                             END OF YOUR CODE                               #\n",
        "##############################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6Dt3bN836Yn"
      },
      "outputs": [],
      "source": [
        "# Run this cell to visualize training loss and train / val accuracy\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.title('Training loss')\n",
        "plt.plot(solver.loss_history, 'o')\n",
        "plt.xlabel('Iteration')\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.title('Accuracy')\n",
        "plt.plot(solver.train_acc_history, '-o', label='train')\n",
        "plt.plot(solver.val_acc_history, '-o', label='val')\n",
        "plt.plot([0.5] * len(solver.val_acc_history), 'k--')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(loc='lower right')\n",
        "plt.gcf().set_size_inches(15, 12)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFpMK72r36Yn"
      },
      "source": [
        "<font size='4' color='red'>**Task 2.3: Multilayer network (18 points).**\n",
        "    \n",
        "<font size='4'>Next you will implement a fully-connected network with an arbitrary number of hidden layers.\n",
        "\n",
        "<font size='4'>Implement the initialization, the forward pass, and the backward pass."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbBqKWK736Yn"
      },
      "outputs": [],
      "source": [
        "class FullyConnectedNet(object):\n",
        "    \"\"\"\n",
        "    A fully-connected neural network with an arbitrary number of hidden layers,\n",
        "    ReLU nonlinearities, and a softmax loss function. For a network with L layers,\n",
        "    the architecture will be\n",
        "\n",
        "    {affine - relu} x (L - 1) - affine - softmax\n",
        "\n",
        "    where the {affine - relu} block is repeated L - 1 times.\n",
        "\n",
        "    Similar to the TwoLayerNet above, learnable parameters are stored in the\n",
        "    self.params dictionary and will be learned using the Solver class.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_dims, input_dim=3*32*32, num_classes=10, reg=0.0,\n",
        "                 weight_scale=1e-2, dtype=np.float32, seed=None):\n",
        "        \"\"\"\n",
        "        Initialize a new FullyConnectedNet.\n",
        "\n",
        "        Inputs:\n",
        "        - hidden_dims: A list of integers giving the size of each hidden layer.\n",
        "        - input_dim: An integer giving the size of the input.\n",
        "        - num_classes: An integer giving the number of classes to classify.\n",
        "        - reg: Scalar giving L2 regularization strength.\n",
        "        - weight_scale: Scalar giving the standard deviation for random\n",
        "          initialization of the weights.\n",
        "        - dtype: A numpy datatype object; all computations will be performed using\n",
        "          this datatype. float32 is faster but less accurate, so you should use\n",
        "          float64 for numeric gradient checking.\n",
        "        - seed: Random seed.\n",
        "        \"\"\"\n",
        "        self.reg = reg\n",
        "        self.num_layers = 1 + len(hidden_dims)\n",
        "        self.dtype = dtype\n",
        "        self.params = {}\n",
        "\n",
        "        ############################################################################\n",
        "        # TODO: Initialize the parameters of the network, storing all values in    #\n",
        "        # the self.params dictionary. Store weights and biases for the first layer #\n",
        "        # in W1 and b1; for the second layer use W2 and b2, etc. Weights should be #\n",
        "        # initialized from a normal distribution centered at 0 with standard       #\n",
        "        # deviation equal to weight_scale. Biases should be initialized to zero.   #\n",
        "        ############################################################################\n",
        "\n",
        "        raise NotImplementedError\n",
        "\n",
        "        ############################################################################\n",
        "        #                             END OF YOUR CODE                             #\n",
        "        ############################################################################\n",
        "\n",
        "        # Cast all parameters to the correct datatype\n",
        "        for k, v in self.params.items():\n",
        "            self.params[k] = v.astype(dtype)\n",
        "\n",
        "\n",
        "    def loss(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Compute loss and gradient for the fully-connected net.\n",
        "\n",
        "        Input / output: Same as TwoLayerNet above.\n",
        "        \"\"\"\n",
        "        X = X.astype(self.dtype)\n",
        "        mode = 'test' if y is None else 'train'\n",
        "\n",
        "        scores = None\n",
        "        ############################################################################\n",
        "        # TODO: Implement the forward pass for the fully-connected net, computing  #\n",
        "        # the class scores for X and storing them in the scores variable.          #\n",
        "        ############################################################################\n",
        "\n",
        "        raise NotImplementedError\n",
        "\n",
        "        ############################################################################\n",
        "        #                             END OF YOUR CODE                             #\n",
        "        ############################################################################\n",
        "\n",
        "        # If test mode return early\n",
        "        if mode == 'test':\n",
        "            return scores\n",
        "\n",
        "        loss, grads = 0.0, {}\n",
        "        ############################################################################\n",
        "        # TODO: Implement the backward pass for the fully-connected net. Store the #\n",
        "        # loss in the loss variable and gradients in the grads dictionary. Compute #\n",
        "        # data loss using softmax, and make sure that grads[k] holds the gradients #\n",
        "        # for self.params[k]. Don't forget to add L2 regularization!               #\n",
        "        #                                                                          #\n",
        "        # NOTE: To ensure that your implementation matches ours and you pass the   #\n",
        "        # automated tests, make sure that your L2 regularization includes a factor #\n",
        "        # of 0.5 to simplify the expression for the gradient. It's sufficient to   #\n",
        "        # compute the L2 regularization for the weights (W1, W2, ...) only.        #\n",
        "        ############################################################################\n",
        "\n",
        "        raise NotImplementedError\n",
        "\n",
        "        ############################################################################\n",
        "        #                             END OF YOUR CODE                             #\n",
        "        ############################################################################\n",
        "\n",
        "        return loss, grads\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCNzDwqt36Yn"
      },
      "outputs": [],
      "source": [
        "np.random.seed(231)\n",
        "N, D, H1, H2, C = 2, 15, 20, 30, 10\n",
        "X = np.random.randn(N, D)\n",
        "y = np.random.randint(C, size=(N,))\n",
        "\n",
        "for reg in [0, 3.14]:\n",
        "    print('Running check with reg = ', reg)\n",
        "    model = FullyConnectedNet(\n",
        "        [H1, H2],\n",
        "        input_dim=D,\n",
        "        num_classes=C,\n",
        "        reg=reg,\n",
        "        weight_scale=5e-2,\n",
        "        dtype=np.float64\n",
        "    )\n",
        "\n",
        "    loss, grads = model.loss(X, y)\n",
        "    print('Initial loss: ', loss)\n",
        "\n",
        "    # Most of the errors should be on the order of 1e-7 or smaller.\n",
        "    # NOTE: It is fine however to see an error for W2 on the order of 1e-5\n",
        "    # for the check when reg = 0.0\n",
        "    for name in sorted(grads):\n",
        "        f = lambda _: model.loss(X, y)[0]\n",
        "        grad_num = eval_numerical_gradient(f, model.params[name], verbose=False, h=1e-5)\n",
        "        print('%s relative error: %.2e' % (name, rel_error(grad_num, grads[name])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRU-XXaR36Yn"
      },
      "source": [
        "<font size='4' color='red'>**Task 2.4: Overfitting a small dataset (5 points).**\n",
        "\n",
        "<font size='4'>As another sanity check, make sure you can overfit a small dataset of 50 images. First we will try a three-layer network with 100 units in each hidden layer. In the following cell, tweak the learning rate and initialization scale to overfit and achieve 100% training accuracy within 20 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "RBuccAbP36Yn"
      },
      "outputs": [],
      "source": [
        "num_train = 50\n",
        "small_data = {\n",
        "  'X_train': data['X_train'][:num_train],\n",
        "  'y_train': data['y_train'][:num_train],\n",
        "  'X_val': data['X_val'],\n",
        "  'y_val': data['y_val'],\n",
        "}\n",
        "\n",
        "\n",
        "#############################################################\n",
        "# tweaking just the learning rate and initialization scale.\n",
        "#############################################################\n",
        "\n",
        "weight_scale = 1\n",
        "learning_rate = 1\n",
        "\n",
        "#############################################################\n",
        "#                 end of your tweaking                      #\n",
        "#############################################################\n",
        "\n",
        "model = FullyConnectedNet(\n",
        "    [100, 100],\n",
        "    weight_scale=weight_scale,\n",
        "    dtype=np.float64\n",
        "\n",
        ")\n",
        "solver = Solver(\n",
        "    model, small_data,\n",
        "    print_every=10, num_epochs=20, batch_size=25,\n",
        "    update_rule='sgd',\n",
        "    optim_config={'learning_rate': learning_rate,}\n",
        ")\n",
        "solver.train()\n",
        "\n",
        "plt.plot(solver.loss_history, 'o')\n",
        "plt.title('Training loss history')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Training loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "546630K036Yo"
      },
      "source": [
        "<font size='4' color='red'>**Task 2.5: Overfitting a small dataset with a five-layer network (5 points).**\n",
        "    \n",
        "<font size='4'>Now try to use a five-layer network with 100 units on each layer to overfit 50 training examples. Again you will have to adjust the learning rate and weight initialization, but you should be able to achieve 100% training accuracy within 20 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLkFxUeD36Yo"
      },
      "outputs": [],
      "source": [
        "num_train = 50\n",
        "small_data = {\n",
        "  'X_train': data['X_train'][:num_train],\n",
        "  'y_train': data['y_train'][:num_train],\n",
        "  'X_val': data['X_val'],\n",
        "  'y_val': data['y_val'],\n",
        "}\n",
        "\n",
        "\n",
        "#############################################################\n",
        "# tweaking just the learning rate and initialization scale.\n",
        "#############################################################\n",
        "\n",
        "weight_scale = 1\n",
        "learning_rate = 1\n",
        "\n",
        "#############################################################\n",
        "#                 end of your tweaking                      #\n",
        "#############################################################\n",
        "\n",
        "model = FullyConnectedNet(\n",
        "    [100, 100, 100, 100],\n",
        "    weight_scale=weight_scale,\n",
        "    dtype=np.float64\n",
        ")\n",
        "\n",
        "solver = Solver(\n",
        "    model, small_data,\n",
        "    print_every=10, num_epochs=20, batch_size=25,\n",
        "    update_rule='sgd',\n",
        "    optim_config={'learning_rate': learning_rate,}\n",
        ")\n",
        "solver.train()\n",
        "\n",
        "plt.plot(solver.loss_history, 'o')\n",
        "plt.title('Training loss history')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Training loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8681C8_36Yo"
      },
      "source": [
        "<font size='4' color='red'>**Inline Question 2.6 (2 points):**\n",
        "    \n",
        "<font size='4' color='red'>**Did you notice anything about the comparative difficulty of training the three-layer net vs training the five layer net? In particular, based on your experience, which network seemed more sensitive to the initialization scale? Why do you think that is the case?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1ZJrPmc36Yp"
      },
      "source": [
        "<font size='4'>**Answer:**\n",
        "    \n",
        "[FILL THIS IN]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXwpo4t736Yp"
      },
      "source": [
        "<font size='4' color='red'>**Task 2.7: SGD+Momentum (5 points)**\n",
        "    \n",
        "<font size='4'>Stochastic gradient descent with momentum is a widely used update rule that tends to make deep networks converge faster than vanilla stochastic gradient descent.\n",
        "\n",
        "<font size='4'>Open the file `utils/optim.py` and read the documentation at the top of the file to make sure you understand the API. Implement the SGD+momentum update rule below and run the following to check your implementation. You should see errors less than 1e-8."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKEf8LCm36Yp"
      },
      "outputs": [],
      "source": [
        "def sgd_momentum(w, dw, config=None):\n",
        "    \"\"\"\n",
        "    Performs stochastic gradient descent with momentum.\n",
        "\n",
        "    config format:\n",
        "    - learning_rate: Scalar learning rate.\n",
        "    - momentum: Scalar between 0 and 1 giving the momentum value.\n",
        "      Setting momentum = 0 reduces to sgd.\n",
        "    - velocity: A numpy array of the same shape as w and dw used to store a\n",
        "      moving average of the gradients.\n",
        "    \"\"\"\n",
        "    if config is None: config = {}\n",
        "    config.setdefault('learning_rate', 1e-2)\n",
        "    config.setdefault('momentum', 0.9)\n",
        "    v = config.get('velocity', np.zeros_like(w))\n",
        "\n",
        "    next_w = None\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the momentum update formula. Store the updated value in #\n",
        "    # the next_w variable. You should also use and update the velocity v.     #\n",
        "    ###########################################################################\n",
        "\n",
        "    raise NotImplementedError\n",
        "\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    config['velocity'] = v\n",
        "\n",
        "    return next_w, config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvBphDmG36Yp"
      },
      "outputs": [],
      "source": [
        "# Let's check your implementation\n",
        "N, D = 4, 5\n",
        "w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n",
        "dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n",
        "v = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n",
        "\n",
        "config = {'learning_rate': 1e-3, 'velocity': v}\n",
        "next_w, _ = sgd_momentum(w, dw, config=config)\n",
        "\n",
        "expected_next_w = np.asarray([\n",
        "  [ 0.1406,      0.20738947,  0.27417895,  0.34096842,  0.40775789],\n",
        "  [ 0.47454737,  0.54133684,  0.60812632,  0.67491579,  0.74170526],\n",
        "  [ 0.80849474,  0.87528421,  0.94207368,  1.00886316,  1.07565263],\n",
        "  [ 1.14244211,  1.20923158,  1.27602105,  1.34281053,  1.4096    ]])\n",
        "expected_velocity = np.asarray([\n",
        "  [ 0.5406,      0.55475789,  0.56891579, 0.58307368,  0.59723158],\n",
        "  [ 0.61138947,  0.62554737,  0.63970526,  0.65386316,  0.66802105],\n",
        "  [ 0.68217895,  0.69633684,  0.71049474,  0.72465263,  0.73881053],\n",
        "  [ 0.75296842,  0.76712632,  0.78128421,  0.79544211,  0.8096    ]])\n",
        "\n",
        "# Should see relative errors around e-8 or less\n",
        "print('next_w error: ', rel_error(next_w, expected_next_w))\n",
        "print('velocity error: ', rel_error(expected_velocity, config['velocity']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "DtK2FicR36Yp"
      },
      "outputs": [],
      "source": [
        "# Run the following to train a six-layer network with both SGD and SGD+momentum.\n",
        "# You should see the SGD+momentum update rule converge faster.\n",
        "from utils.solver import Solver\n",
        "\n",
        "num_train = 4000\n",
        "small_data = {\n",
        "  'X_train': data['X_train'][:num_train],\n",
        "  'y_train': data['y_train'][:num_train],\n",
        "  'X_val': data['X_val'],\n",
        "  'y_val': data['y_val'],\n",
        "}\n",
        "\n",
        "solvers = {}\n",
        "\n",
        "for update_rule in ['sgd', 'sgd_momentum']:\n",
        "    print('running with ', update_rule)\n",
        "    model = FullyConnectedNet([100, 100, 100, 100, 100], weight_scale=5e-2)\n",
        "\n",
        "    solver = Solver(\n",
        "        model, small_data,\n",
        "        num_epochs=5, batch_size=100,\n",
        "        update_rule=update_rule if update_rule == 'sgd' else sgd_momentum,\n",
        "        optim_config={'learning_rate': 1e-2,},\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    solvers[update_rule] = solver\n",
        "    solver.train()\n",
        "    print()\n",
        "\n",
        "plt.subplot(3, 1, 1)\n",
        "plt.title('Training loss')\n",
        "plt.xlabel('Iteration')\n",
        "\n",
        "plt.subplot(3, 1, 2)\n",
        "plt.title('Training accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "\n",
        "plt.subplot(3, 1, 3)\n",
        "plt.title('Validation accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "\n",
        "for update_rule, solver in list(solvers.items()):\n",
        "    plt.subplot(3, 1, 1)\n",
        "    plt.plot(solver.loss_history, 'o', label=update_rule)\n",
        "\n",
        "    plt.subplot(3, 1, 2)\n",
        "    plt.plot(solver.train_acc_history, '-o', label=update_rule)\n",
        "\n",
        "    plt.subplot(3, 1, 3)\n",
        "    plt.plot(solver.val_acc_history, '-o', label=update_rule)\n",
        "\n",
        "for i in [1, 2, 3]:\n",
        "    plt.subplot(3, 1, i)\n",
        "    plt.legend(loc='upper center', ncol=4)\n",
        "plt.gcf().set_size_inches(15, 15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EppKrYo36Yp"
      },
      "source": [
        "<font size='4' color='red'>**Task 2.8: Train a good model! (7 points)**\n",
        "    \n",
        "<font size='4'>Train the best fully-connected model that you can on CIFAR-10, storing your best model in the `best_model` variable. <font color='red'>We require you to get at least 50% accuracy on the validation set using a fully-connected net within 50 epochs.\n",
        "\n",
        "<font size='4'>If you are careful it should be possible to get accuracies above 55%, but we don't require it for this part and won't assign extra credit for doing so."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "8lz9nPza36Yp"
      },
      "outputs": [],
      "source": [
        "best_model = None\n",
        "model = None\n",
        "################################################################################\n",
        "# TODO: Train the best FullyConnectedNet that you can on CIFAR-10. Your best   #\n",
        "# model will be stored in the best_model variable.                             #\n",
        "################################################################################\n",
        "\n",
        "raise NotImplementedError\n",
        "\n",
        "################################################################################\n",
        "#                              END OF YOUR CODE                                #\n",
        "################################################################################\n",
        "\n",
        "best_model = model\n",
        "best_model.params.update(solver.best_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-Lch6cc36Yp"
      },
      "outputs": [],
      "source": [
        "# Run your best model on the validation and test sets. You should achieve above 50% accuracy on the validation set.\n",
        "y_test_pred = np.argmax(best_model.loss(data['X_test']), axis=1)\n",
        "y_val_pred = np.argmax(best_model.loss(data['X_val']), axis=1)\n",
        "print('Validation set accuracy: ', (y_val_pred == data['y_val']).mean())\n",
        "print('Test set accuracy: ', (y_test_pred == data['y_test']).mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQbTDZpF36Yp"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}