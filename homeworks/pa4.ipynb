{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "YsdSwQFUzayK",
      "metadata": {
        "id": "YsdSwQFUzayK"
      },
      "source": [
        "# Programming Assignment 4: Sequence-to-sequence Generation with Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UG-Art03zayL",
      "metadata": {
        "id": "UG-Art03zayL"
      },
      "source": [
        "## Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "knGEE-F0zayL",
      "metadata": {
        "id": "knGEE-F0zayL"
      },
      "source": [
        "<font size='4'>In this assignment you will practice implementing a Transformer-based sequence-to-sequence (seq2seq) generation model using both Transformer encoder and decoder for machine translation (from French to English). On the one hand, you need to implement a custom Transformer decoder. On the other hand, you will also learn how to use PyTorch's built-in multi-head attention and Transformer encoder modules. In this way, you not only deepen your understanding of Transformer by implementing it on your own but also learn how to construct a model for a downstream application using PyTorch's built-in modules. After finishing this programming assignment, you will get good understandings about foundations for very state-of-the-art models that you likely see in tech news articles nowadays, like ChatGPT.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c9Hjab-zayL",
      "metadata": {
        "id": "5c9Hjab-zayL"
      },
      "source": [
        "## Submission format"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b502_m6FzayM",
      "metadata": {
        "id": "b502_m6FzayM"
      },
      "source": [
        "- <font size='4'>`<your_nu_username>_pa4.ipynb` with your implementations and output.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ovlu7pv1zayM",
      "metadata": {
        "id": "ovlu7pv1zayM"
      },
      "source": [
        "## Note"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2MCxCtR3zayM",
      "metadata": {
        "id": "2MCxCtR3zayM"
      },
      "source": [
        "<font size='4'>  \n",
        "\n",
        "- **Read the instructions and comments very carefully to avoid waste of your valuable time and deductions of points.**\n",
        "\n",
        "- You do not install any additional packages inside the Colab environment. Do not forget to choose to use GPU in the `Runtime\\Change runtime type` tab.    \n",
        "\n",
        "- **You are not allowed to look for answers online, except for the links provided in this assignment.**\n",
        "\n",
        "- **Violation of this policy will lead to failure of your course and even more severe consequences.**\n",
        "\n",
        "- Attend office hours and make posts on Piazza if you have any questions.\n",
        "</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c34a5320",
      "metadata": {
        "id": "c34a5320"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gBalaRBDzayM",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBalaRBDzayM",
        "outputId": "5358ae36-0dd8-41d1-928e-c6d2d10bce5c"
      },
      "outputs": [],
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import re\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# We will be using the official implementation of the multi-head attention\n",
        "from torch.nn import MultiheadAttention, TransformerEncoder\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('We are using the device {}.'.format(device))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98374904",
      "metadata": {
        "id": "98374904"
      },
      "source": [
        "## Data Preparation\n",
        "\n",
        "<font size='4'> The data preparation part is largely borrowed (with modifications) from an online tutorial https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html. You are encouraged to read it and allowed to take inspirations from their implementations of the RNN-based seq2seq model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4708408d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4708408d",
        "outputId": "2253d784-42b9-4fe8-8aab-77477ffafcfa"
      },
      "outputs": [],
      "source": [
        "# Download the data\n",
        "!wget https://download.pytorch.org/tutorial/data.zip\n",
        "!unzip -o data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c7a2f06c",
      "metadata": {
        "id": "c7a2f06c"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "We'll need a unique index per word to use as the inputs and targets of the networks later.\n",
        "To keep track of all this we will use a helper class called Lang which has\n",
        "word → index (word2index) and index → word (index2word) dictionaries, as well as a count\n",
        "of each word word2count which will be used to replace rare words later.\n",
        "\"\"\"\n",
        "\n",
        "# Define two special tokens here\n",
        "# SOS - start of a sentence\n",
        "# EOS - end of a sentence\n",
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "# Note that the PAD token is not defined here, which may affect the performance, but not critically.\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        # SOS - start of sentence\n",
        "        # EOS - end of sentence\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "57b57a04",
      "metadata": {
        "id": "57b57a04"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "The files are all in Unicode, to simplify we will turn Unicode\n",
        "characters to ASCII, make everything lowercase, and trim most\n",
        "punctuation.\n",
        "\"\"\"\n",
        "\n",
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# https://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
        "    return s.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "eaa35885",
      "metadata": {
        "id": "eaa35885"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "To read the data file we will split the file into lines, and then split lines into pairs.\n",
        "The files are all English → Other Language, so if we want to translate from\n",
        "Other Language → English I added the reverse flag to reverse the pairs.\n",
        "\"\"\"\n",
        "\n",
        "def readLangs(lang1, lang2, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "\n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "80c68304",
      "metadata": {
        "id": "80c68304"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Since there are a lot of example sentences and we want to train something quickly,\n",
        "we'll trim the data set to only relatively short and simple sentences.\n",
        "Here the maximum length is 10 words (that includes ending punctuation) and\n",
        "we're filtering to sentences that translate to the form \"I am\" or \"He is\" etc.\n",
        "(accounting for apostrophes replaced earlier).\n",
        "\"\"\"\n",
        "\n",
        "MAX_LENGTH = 10\n",
        "\n",
        "ENG_PREFIXES = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s \",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \"\n",
        ")\n",
        "\n",
        "def filterPair(p, max_length, eng_prefixes=None):\n",
        "    condition = len(p[0].split(' ')) < max_length and \\\n",
        "        len(p[1].split(' ')) < max_length\n",
        "    if eng_prefixes is not None:\n",
        "        condition = condition and p[1].startswith(eng_prefixes)\n",
        "    return condition\n",
        "\n",
        "def filterPairs(pairs, max_length, eng_prefixes=None):\n",
        "    return [pair for pair in pairs if filterPair(pair, max_length, eng_prefixes)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03c69d2d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03c69d2d",
        "outputId": "aadef5c4-f138-4191-b2f5-af0a25469e44"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "The full process for preparing the data is:\n",
        "\n",
        "- Read text file and split into lines, split lines into pairs\n",
        "- Normalize text, filter by length and content\n",
        "- Make word lists from sentences in pairs\n",
        "\"\"\"\n",
        "\n",
        "def prepareData(lang1, lang2, max_length, eng_prefixes=ENG_PREFIXES, reverse=False):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filterPairs(pairs, max_length, eng_prefixes)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "# Let's see what the data looks like\n",
        "input_lang, output_lang, pairs = prepareData('eng', 'fra', MAX_LENGTH, ENG_PREFIXES, True)\n",
        "print(random.choice(pairs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "9e491072",
      "metadata": {
        "id": "9e491072"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "To train, for each pair we will need an input tensor (indexes of the words in\n",
        "the input sentence) and target tensor (indexes of the words in the target sentence).\n",
        "While creating these vectors we will append the EOS token to both sequences.\n",
        "\"\"\"\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
        "\n",
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)\n",
        "\n",
        "def convert_pairs_to_dataloader(pairs, batch_size, is_train):\n",
        "    n = len(pairs)\n",
        "    input_ids = np.ones((n, MAX_LENGTH), dtype=np.int32) * EOS_token\n",
        "    target_ids = np.ones((n, MAX_LENGTH + 1), dtype=np.int32) * EOS_token\n",
        "\n",
        "    for idx, (inp, tgt) in enumerate(pairs):\n",
        "        inp_ids = indexesFromSentence(input_lang, inp)\n",
        "        tgt_ids = indexesFromSentence(output_lang, tgt)\n",
        "        inp_ids.append(EOS_token)\n",
        "        tgt_ids.append(EOS_token)\n",
        "        input_ids[idx, :len(inp_ids)] = inp_ids\n",
        "        target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
        "\n",
        "    data = TensorDataset(torch.LongTensor(input_ids).to(device),\n",
        "                         torch.LongTensor(target_ids).to(device))\n",
        "\n",
        "    if is_train:\n",
        "        sampler = RandomSampler(data)\n",
        "        dataloader = DataLoader(data, sampler=sampler, batch_size=batch_size, drop_last=True)#, num_workers=2)\n",
        "    else:\n",
        "        dataloader = DataLoader(data, sampler=None, batch_size=batch_size, drop_last=False)#, num_workers=2)\n",
        "    return dataloader\n",
        "\n",
        "def get_dataloader(pairs, batch_size):\n",
        "    np.random.shuffle(pairs)\n",
        "\n",
        "    n = len(pairs)\n",
        "    n_train = int(n * 0.9)\n",
        "    train_pairs = pairs[:n_train]\n",
        "    test_pairs = pairs[n_train:]\n",
        "    train_dataloader = convert_pairs_to_dataloader(train_pairs, batch_size, True)\n",
        "    test_dataloader = convert_pairs_to_dataloader(test_pairs, batch_size, False)\n",
        "\n",
        "    return train_pairs, train_dataloader, test_pairs, test_dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d20018d",
      "metadata": {
        "id": "2d20018d"
      },
      "source": [
        "## Auxiliary Modules and Functions\n",
        "\n",
        "<font size='4'>You have implemented some of them in the third programming assignment, which are needed for this assignment, too. So their reference implementations are provided here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "86b562ef",
      "metadata": {
        "id": "86b562ef"
      },
      "outputs": [],
      "source": [
        "class FeedForwardNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    A simple feedforward network. Essentially, it is a two-layer fully-connected\n",
        "    neural network.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, ff_dim, dropout):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "        - input_dim: Input dimension\n",
        "        - ff_dim: Hidden dimension\n",
        "        \"\"\"\n",
        "        super(FeedForwardNetwork, self).__init__()\n",
        "\n",
        "        ###########################################################################\n",
        "        # TODO: Define the two linear layers and a non-linear one.\n",
        "        ###########################################################################\n",
        "        self.w1 = nn.Linear(input_dim, ff_dim)\n",
        "        self.w2 = nn.Linear(ff_dim, input_dim)\n",
        "        self.non_linear = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        ###########################################################################\n",
        "        #                             END OF YOUR CODE                            #\n",
        "        ###########################################################################\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "        - x: Tensor of the shape BxLxC, where B is the batch size, L is the sequence length,\n",
        "         and C is the channel dimension\n",
        "\n",
        "        Return:\n",
        "        - y: Tensor of the shape BxLxC\n",
        "        \"\"\"\n",
        "\n",
        "        y = None\n",
        "        ###########################################################################\n",
        "        # TODO: Process the input.                                                #\n",
        "        ###########################################################################\n",
        "        y = self.dropout(self.non_linear(self.w1(x)))\n",
        "        y = self.w2(y)\n",
        "        ###########################################################################\n",
        "        #                             END OF YOUR CODE                            #\n",
        "        ###########################################################################\n",
        "\n",
        "        return y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "3028c0af",
      "metadata": {
        "id": "3028c0af"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    A module that adds positional encoding to each of the token's features.\n",
        "    So that the Transformer is position aware.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim: int, max_len: int=10000):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "        - input_dim: Input dimension about the features for each token\n",
        "        - max_len: The maximum sequence length\n",
        "        \"\"\"\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Compute the positional encoding and add it to x.\n",
        "\n",
        "        Input:\n",
        "        - x: Tensor of the shape BxLxC, where B is the batch size, L is the sequence length,\n",
        "          and C is the channel dimension\n",
        "\n",
        "        Return:\n",
        "        - x: Tensor of the shape BxLxC, with the positional encoding added to the input\n",
        "        \"\"\"\n",
        "        seq_len = x.shape[1]\n",
        "        input_dim = x.shape[2]\n",
        "\n",
        "        pe = None\n",
        "        ###########################################################################\n",
        "        # TODO: Compute the positional encoding                                   #\n",
        "        # Check Section 3.5 for the definition (https://arxiv.org/pdf/1706.03762.pdf)\n",
        "        #                                                                         #\n",
        "        # It's a bit messy, but the definition is provided for your here for your #\n",
        "        # convenience (in LaTex).                                                 #\n",
        "        # PE_{(pos,2i)} = sin(pos / 10000^{2i/\\dmodel}) \\\\                        #\n",
        "        # PE_{(pos,2i+1)} = cos(pos / 10000^{2i/\\dmodel})                         #\n",
        "        #                                                                         #\n",
        "        # You should replace 10000 with max_len here.\n",
        "        ###########################################################################\n",
        "        norm = 10000.0 ** (torch.arange(0, self.input_dim, 2) / input_dim)\n",
        "        pos = torch.arange(seq_len).unsqueeze(1)\n",
        "        pe = torch.zeros(seq_len, input_dim)\n",
        "        pe[:, ::2]  = torch.sin(pos / norm)\n",
        "        pe[:, 1::2] = torch.cos(pos / norm)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        ###########################################################################\n",
        "        #                             END OF YOUR CODE                            #\n",
        "        ###########################################################################\n",
        "\n",
        "        x = x + pe.to(x.device)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a21c51ff",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "id": "a21c51ff",
        "outputId": "5db31ca4-d21e-447f-ddf3-4bb5b2257466"
      },
      "outputs": [],
      "source": [
        "# Create and visualize the causal mask that is needed for training of the decoder\n",
        "# in a Transformer-based seq2seq model.\n",
        "\n",
        "def create_causal_mask(size):\n",
        "    \"Mask out subsequent positions. Also known as a causual mask.\"\n",
        "    attn_shape = (size, size)\n",
        "    causal_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
        "    return torch.from_numpy(causal_mask) == 0\n",
        "\n",
        "# Let's visualize what the target mask looks like\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn\n",
        "seaborn.set_context(context=\"talk\")\n",
        "\n",
        "causal_mask = create_causal_mask(20)\n",
        "print(causal_mask.shape)\n",
        "\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.imshow(causal_mask.numpy())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DIzE2MiBzayN",
      "metadata": {
        "id": "DIzE2MiBzayN"
      },
      "source": [
        "<font size='4' color='red'> Your implementations start from here.</font>\n",
        "\n",
        "## Part 1: Implementation of the Transformer decoder and seq2seq model (60 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dlBIb4p_zayN",
      "metadata": {
        "id": "dlBIb4p_zayN"
      },
      "source": [
        "### <font size='4' color='red'>Task 1.1: Implement Transformer Decoder Cell (15 points)</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "okKAcRPNzayN",
      "metadata": {
        "id": "okKAcRPNzayN"
      },
      "outputs": [],
      "source": [
        "class MyTransformerDecoderCell(nn.Module):\n",
        "    \"\"\"\n",
        "    A single cell (unit) of the Transformer decoder.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim: int, num_heads: int, ff_dim: int, dropout: float=0.1):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "        - input_dim: Input dimension for each token in a sequence\n",
        "        - num_heads: Number of attention heads in a multi-head attention module\n",
        "        - ff_dim: The hidden dimension for a feedforward network\n",
        "        - dropout: Dropout ratio for the output of the multi-head attention and feedforward\n",
        "          modules.\n",
        "        \"\"\"\n",
        "        super(MyTransformerDecoderCell, self).__init__()\n",
        "\n",
        "        ###########################################################################\n",
        "        # TODO: Define two multi-head attention modules using                     #\n",
        "        # nn.MultiheadAttention. See the API definition at                        #\n",
        "        # https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html#\n",
        "        # Note that you need to set batch_first=True. Out of the two multi-head   #\n",
        "        # attention modules, one is for processing the tokens on the decoder side,#\n",
        "        # dong self attention. The other is doing cross attention, getting the    #\n",
        "        # attention from the decoder embeddings to the encoder embeddings. Also   #\n",
        "        # define a feedforward network. Don't forget the Dropout and LayerNorm    #\n",
        "        # layers.                                                                 #\n",
        "        #                                                                         #\n",
        "        # Note that you need to implement the post-norm variant here.             #\n",
        "        ###########################################################################\n",
        "        raise NotImplementedError\n",
        "        ###########################################################################\n",
        "        #                             END OF YOUR CODE                            #\n",
        "        ###########################################################################\n",
        "\n",
        "    def forward(self, x: torch.Tensor, encoder_output: torch.Tensor, tgt_mask=None, tgt_is_causal=False):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "        - x: Tensor of BxLdxC, word embeddings on the decoder side\n",
        "        - encoder_output: Tensor of BxLexC, word embeddings on the encoder side\n",
        "        - tgt_mask: Tensor, causal mask for the self attention of the tokens on the\n",
        "          decoder side, which prevents each token to attend to future tokens\n",
        "        - tgt_is_causal: Boolean, indicating whether the tgt_mask is causal or not.\n",
        "          If a causal mask is provided, make sure setting tgt_is_causal=True when calling\n",
        "          the self attention module. These arguments are defined in accordance with the\n",
        "          interface of nn.TransformerDecoderCell so you can use it for debugging purpose.\n",
        "\n",
        "        Return:\n",
        "        - y: Tensor of BxLdxC. Attended features for all tokens on the decoder side.\n",
        "        \"\"\"\n",
        "\n",
        "        y = None\n",
        "        ###########################################################################\n",
        "        # TODO: Compute the self-attended features for the tokens on the decoder  #\n",
        "        # side. Then compute the cross-attended features for the tokens on the    #\n",
        "        # decoder side to the encoded features, which are finally fed into the    #\n",
        "        # feedforward network.                                                    #\n",
        "        ###########################################################################\n",
        "        raise NotImplementedError\n",
        "        ###########################################################################\n",
        "        #                             END OF YOUR CODE                            #\n",
        "        ###########################################################################\n",
        "\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "diDugP_joFSn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "diDugP_joFSn",
        "outputId": "a2e385d3-1aa9-427b-c9e9-616178f53d62"
      },
      "outputs": [],
      "source": [
        "# Sanity check with a decoder causal mask\n",
        "dec_embed = torch.randn((3, 10, 16))\n",
        "dec_causal_mask = create_causal_mask(10) # why 10 here?\n",
        "\n",
        "enc_embed = torch.randn((3, 12, 16))\n",
        "\n",
        "model = MyTransformerDecoderCell(16, 2, 32, 0.1)\n",
        "z = model(dec_embed, enc_embed, dec_causal_mask, tgt_is_causal=True)\n",
        "assert len(z.shape) == len(dec_embed.shape)\n",
        "for dim_z, dim_x in zip(z.shape, dec_embed.shape):\n",
        "    assert dim_z == dim_x\n",
        "print('output shape: ', z.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BvDmzOVPzayO",
      "metadata": {
        "id": "BvDmzOVPzayO"
      },
      "source": [
        "### <font size='4' color='red'>Task 1.2: Implement Transformer Decoder (12 points)</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "KDFAYXIjzayO",
      "metadata": {
        "id": "KDFAYXIjzayO"
      },
      "outputs": [],
      "source": [
        "class MyTransformerDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    A MyTransformerDecoder is a stack of multiple MyTransformerDecoderCells and a Layer Norm.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim: int, num_heads: int, ff_dim: int, num_cells: int, dropout=0.1):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "        - input_dim: Input dimension for each token in a sequence\n",
        "        - num_heads: Number of attention heads in a multi-head attention module\n",
        "        - ff_dim: The hidden dimension for a feedforward network\n",
        "        - num_cells: How many MyTransformerDecoderCells in stack\n",
        "        - dropout: Dropout ratio for the output of the multi-head attention and feedforward\n",
        "          modules.\n",
        "        \"\"\"\n",
        "        super(MyTransformerDecoder, self).__init__()\n",
        "\n",
        "        ###########################################################################\n",
        "        # TODO: Construct a nn.ModuleList to store a stack of                     #\n",
        "        # MyTranformerDecoderCells. Check the documentation here of how to use it #\n",
        "        # https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html#torch.nn.ModuleList\n",
        "        #                                                                         #\n",
        "        # At the same time, define a layer normalization layer to process the     #\n",
        "        # output of the entire decoder.                                           #\n",
        "        #                                                                         #\n",
        "        # For debugging purpose, you can replace MyTranformerDecoderCell with the #\n",
        "        # built-in nn.TranformerDecoderCell.                                      #\n",
        "        ###########################################################################\n",
        "        raise NotImplementedError\n",
        "        ###########################################################################\n",
        "        #                             END OF YOUR CODE                            #\n",
        "        ###########################################################################\n",
        "\n",
        "    def forward(self, x: torch.Tensor, encoder_output: torch.Tensor, dec_causal_mask=None):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "        - x: Tensor of BxLdxC, word embeddings on the decoder side\n",
        "        - encoder_output: Tensor of BxLexC, word embeddings on the encoder side\n",
        "        - dec_causal_mask: Tensor, causal mask for the self attention of the tokens on the\n",
        "          decoder side, which prevents each token to attend to future tokens\n",
        "\n",
        "        Return:\n",
        "        - y: Tensor of BxLdxC. Attended features for all tokens on the decoder side.\n",
        "        \"\"\"\n",
        "\n",
        "        y = None\n",
        "        ###########################################################################\n",
        "        # TODO: Feed x into the stack of MyTransformerDecoderCells and then       #\n",
        "        # normalize the final output with layer norm.                             #\n",
        "        #                                                                         #\n",
        "        # Note: when calling TransformerDecoderCell, if a dec_causal_mask is      #\n",
        "        # provided, make sure to set tgt_is_causal=True.                          #\n",
        "        ###########################################################################\n",
        "        raise NotImplementedError\n",
        "        ###########################################################################\n",
        "        #                             END OF YOUR CODE                            #\n",
        "        ###########################################################################\n",
        "\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oJuCzlnSzayO",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJuCzlnSzayO",
        "outputId": "164f8009-7307-47c3-c1cf-b61a1d34c5cb"
      },
      "outputs": [],
      "source": [
        "# Sanity check of tensor shapes\n",
        "dec_embed = torch.randn((3, 10, 16))\n",
        "dec_causal_mask = create_causal_mask(10)\n",
        "\n",
        "enc_embed = torch.randn((3, 12, 16))\n",
        "\n",
        "model = MyTransformerDecoder(16, 2, 32, 2, 0.1)\n",
        "z = model(dec_embed, enc_embed, dec_causal_mask)\n",
        "assert len(z.shape) == len(dec_embed.shape)\n",
        "for dim_z, dim_x in zip(z.shape, dec_embed.shape):\n",
        "    assert dim_z == dim_x\n",
        "print('output shape: ', z.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "718ffe76",
      "metadata": {
        "id": "718ffe76"
      },
      "source": [
        "<font size='4' color='red'>Task 1.3 (inline question): As you can see below, for a Transformer-based seq2seq model, due to the existence of the Transformer decoder, you need to implement `forward_train` and `forward_test`, corresponding its different behaviors in training and testing time. Briefly explain why you need to do so. (8 points) </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27746b76",
      "metadata": {
        "id": "27746b76"
      },
      "source": [
        "[Your answer]: "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Qx2WNnfXzayO",
      "metadata": {
        "id": "Qx2WNnfXzayO"
      },
      "source": [
        "### <font size='4' color='red'>Task 1.4: Implement a Transformer-based sequence-to-sequence model (25 points: 5 for the constructor, 10 for forward_train, and 10 for forward_test)</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "7frR_C_JzayO",
      "metadata": {
        "id": "7frR_C_JzayO"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer-based sequence-to-sequence model.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "            num_encoder_layers: int, num_decoder_layers: int, embed_dim: int,\n",
        "            num_heads: int, src_vocab_size: int, tgt_vocab_size: int,\n",
        "            trx_ff_dim: int = 512, dropout: float = 0.1\n",
        "        ):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "        - num_encoder_layers: How many TransformerEncoderCell in stack\n",
        "        - num_decoder_layers: How many TransformerDecoderCell in stack\n",
        "        - embed_dim: Word embeddings dimension\n",
        "        - num_heads: Number of attention heads\n",
        "        - src_vocab_size: Number of tokens in the source language vocabulary\n",
        "        - tgt_vocab_size: Number of tokens in the target language vocabulary\n",
        "        - trx_ff_dim: Hidden dimension in the feedforward network\n",
        "        - dropout: Dropout ratio\n",
        "        \"\"\"\n",
        "        super(Seq2SeqTransformer, self).__init__()\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        # Word embeddings for both the source and target languages\n",
        "        self.src_token_embed = nn.Embedding(src_vocab_size, embed_dim)\n",
        "        self.tgt_token_embed = nn.Embedding(tgt_vocab_size, embed_dim)\n",
        "        \n",
        "        self.encoder = None\n",
        "        self.decoder = None\n",
        "        self.output_layer = None\n",
        "        ###########################################################################\n",
        "        # TODO: Define the positional encoding module, encoder, decoder, and the  #\n",
        "        # output layer. Think of how many classes are in the output layer.        #\n",
        "        #                                                                         #\n",
        "        # For the encoder, you'll use nn.TransformerEncoderLayer and              #\n",
        "        # nn.TransformerEncoder.  Check                                           #\n",
        "        # https://pytorch.org/docs/stable/nn.html#transformer-layers for how to   #\n",
        "        # use them.                                                               #\n",
        "        #                                                                         #\n",
        "        # For the decoder, you'll use your own MyTransformerDecoder. But for      #\n",
        "        # the debugging purpose, you can use nn.TransformerDecoderLayer and       #\n",
        "        # nn.TransformerDecoder.                                                  #\n",
        "        ###########################################################################\n",
        "        raise NotImplementedError\n",
        "        ###########################################################################\n",
        "        #                             END OF YOUR CODE                            #\n",
        "        ###########################################################################\n",
        "\n",
        "    def forward_train(self, src: torch.Tensor, tgt: torch.Tensor, use_causal_mask=True):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "        - src: Tensor of BxLe, word indexes in the source language\n",
        "        - tgt: Tensor of BxLd, word indexes in the target language\n",
        "        - use_causal_mask: Boolean, indicating whether to use a causal mask in the\n",
        "          decoder. We will use it to examine different behaviors of the model.\n",
        "\n",
        "        Return:\n",
        "        - y: Tensor of BxLdxK, corresponding to the log probabilities of generated\n",
        "             words in the target language. K is the number of classes in the output.\n",
        "        \"\"\"\n",
        "        # Get source language word embeddings. Note they are scaled.\n",
        "        src_embed = self.src_token_embed(src) * math.sqrt(self.embed_dim)\n",
        "\n",
        "        # Append an initial SOS token\n",
        "        init_token = torch.empty(src_embed.shape[0], 1, dtype=torch.long, device=tgt.device).fill_(SOS_token)\n",
        "        tgt_sentence = torch.cat((init_token, tgt), dim=1)\n",
        "        # Get target language word embeddings. Note they are scaled.\n",
        "        tgt_embed = self.tgt_token_embed(tgt_sentence) * math.sqrt(self.embed_dim)\n",
        "\n",
        "        logits = None\n",
        "        ###########################################################################\n",
        "        # TODO: Add positional encodings to the word embeddings of the source and #\n",
        "        # target languages. Then feed them to the encoder and decoder,            #\n",
        "        # respectively and get the logits finally.                                #\n",
        "        #                                                                         #\n",
        "        # Note that no for loop is allowed here.                                  #\n",
        "        ###########################################################################\n",
        "        raise NotImplementedError\n",
        "        ###########################################################################\n",
        "        #                             END OF YOUR CODE                            #\n",
        "        ###########################################################################\n",
        "\n",
        "        # We discard the last token's output (the output of the EOS token), which is meaningless.\n",
        "        logits = logits[:, :-1]\n",
        "\n",
        "        # We compute the log probabilities as we will use nn.NLLLoss() as the loss function\n",
        "        return F.log_softmax(logits, dim=-1)\n",
        "\n",
        "    def forward_test(self, src):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "        - src: Tensor of BxLe, word indexes in the source language\n",
        "\n",
        "        Return:\n",
        "        - y: Tensor of BxLdxK, corresponding to the log probabilities of generated\n",
        "             words in the target language. K is the number of classes in the output.\n",
        "        \"\"\"\n",
        "        # Get source language word embeddings. Note they are scaled.\n",
        "        src_embed = self.src_token_embed(src) * math.sqrt(self.embed_dim)\n",
        "\n",
        "        # For the target language generation, we always start from a SOS token.\n",
        "        decoder_input = torch.empty(src.shape[0], 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
        "\n",
        "        logits = None\n",
        "        ###########################################################################\n",
        "        # TODO: Add positional encodings to the word embeddings of the source     #\n",
        "        # language, from which we will generate one word at a time based on the   #\n",
        "        # previously generated ones (so called autoregressive generation) in the  #\n",
        "        # target language. We will generate MAX_LENGTH + 1 (a global variable     #\n",
        "        # defined earlier) words, **including the EOS token**, at the most using a#\n",
        "        # for loop. You may find the implementation of DecoderRNN in the tutorial #\n",
        "        # https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
        "        # helpful. But be aware of the differences of RNN and Transformer in      #\n",
        "        # terms of autoregressive generation.                                     #\n",
        "        ###########################################################################\n",
        "        raise NotImplementedError\n",
        "        ###########################################################################\n",
        "        #                             END OF YOUR CODE                            #\n",
        "        ###########################################################################\n",
        "\n",
        "        # Keep the returned tensor the same as the training time\n",
        "        return F.log_softmax(logits, dim=-1)\n",
        "\n",
        "    def forward(self, src, tgt=None, use_causal_mask=True):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "        - src: Tensor of BxLe, word indexes in the source language\n",
        "        - tgt: Tensor of BxLd, word indexes in the target language. Can be empty.\n",
        "        - use_causal_mask: Boolean, indicating whether to use a causal mask in the\n",
        "          decoder. We will use it to examine different behaviors of the model.\n",
        "\n",
        "        Return:\n",
        "        - y: Tensor of BxLdxK, corresponding to the log probabilities of generated\n",
        "             words in the target language. K is the number of classes in the output.\n",
        "        \"\"\"\n",
        "        if self.training:\n",
        "            # raining-time behavior\n",
        "            assert tgt is not None\n",
        "            return self.forward_train(src, tgt, use_causal_mask)\n",
        "\n",
        "        # testing-time behavior\n",
        "        return self.forward_test(src)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pXz7nZJazayO",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXz7nZJazayO",
        "outputId": "e4edb824-89a0-4358-ad6c-c7f8bd318edc"
      },
      "outputs": [],
      "source": [
        "# Sanity check of tensor shapes\n",
        "src_vocab_size = 10\n",
        "src = torch.arange(src_vocab_size).view(1, -1)\n",
        "src = torch.cat((src, src), dim=0)\n",
        "\n",
        "tgt_vocab_size = 12\n",
        "tgt = torch.arange(tgt_vocab_size - 3).view(1, -1)\n",
        "tgt = torch.cat((tgt, tgt), dim=0)\n",
        "\n",
        "print('src: ', src.shape)\n",
        "print('tgt: ', tgt.shape)\n",
        "\n",
        "model = Seq2SeqTransformer(2, 2, 16, 2, src_vocab_size, tgt_vocab_size, 32, 0.1)\n",
        "z = model(src, tgt)\n",
        "desired_output_shape = [2, 9, 12]\n",
        "assert len(z.shape) == len(desired_output_shape)\n",
        "for dim_z, dim_x in zip(z.shape, desired_output_shape):\n",
        "    assert dim_z == dim_x\n",
        "print('output shape: ', z.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2b258ec",
      "metadata": {
        "id": "e2b258ec"
      },
      "source": [
        "## Part 2: Train Transformer-based seq2seq models (40 points)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "E9lw033cpQWl",
      "metadata": {
        "id": "E9lw033cpQWl"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This is a helper function to print time elapsed and estimated time\n",
        "remaining given the current time and progress %.\n",
        "\"\"\"\n",
        "\n",
        "import time\n",
        "import math\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "1ohpeW9TpVkN",
      "metadata": {
        "id": "1ohpeW9TpVkN"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "The whole training process looks like this:\n",
        "\n",
        "- Start a timer\n",
        "- Initialize optimizers and criterion\n",
        "- Create set of training pairs\n",
        "- Start empty losses array for plotting\n",
        "Then we call train many times and occasionally print the progress (% of examples, time so far, estimated time) and average loss.\n",
        "\"\"\"\n",
        "# For tips on running notebooks in Google Colab, see\n",
        "# https://pytorch.org/tutorials/beginner/colab\n",
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "id": "09aa4c02",
      "metadata": {
        "id": "09aa4c02"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "To train we run the input sentence through the encoder, and keep track of every\n",
        "output and the latest hidden state. Then the decoder is given the <SOS> token as\n",
        " its first input, and the last hidden state of the encoder as its first hidden state.\n",
        "\n",
        "\"Teacher forcing\" is the concept of using the real target outputs as each next\n",
        "input, instead of using the decoder's guess as the next input. Using teacher\n",
        "forcing causes it to converge faster but when the trained network is exploited,\n",
        "it may exhibit instability.\n",
        "\n",
        "You can observe outputs of teacher-forced networks that read with coherent grammar\n",
        "but wander far from the correct translation -intuitively it has learned to represent\n",
        "the output grammar and can \"pick up\" the meaning once the teacher tells it the\n",
        "first few words, but it has not properly learned how to create the sentence from\n",
        "the translation in the first place.\n",
        "\n",
        "Because of the freedom PyTorch's autograd gives us, we can randomly choose to use\n",
        "teacher forcing or not with a simple if statement. Turn teacher_forcing_ratio\n",
        "up to use more of it.\n",
        "\"\"\"\n",
        "\n",
        "def train_epoch(dataloader, model, optimizer, criterion, lr_scheduler, use_causal_mask):\n",
        "\n",
        "    total_loss = 0\n",
        "    for data in dataloader:\n",
        "        input_tensor, target_tensor = data\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        decoder_outputs = model(input_tensor, target_tensor, use_causal_mask)\n",
        "\n",
        "        # The neg-log likelihood loss of language generation is also known as perplexity.\n",
        "        loss = criterion(\n",
        "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
        "            target_tensor.view(-1)\n",
        "        )\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "def train(train_dataloader, model, n_epochs, optimizer, lr_scheduler, use_causal_mask, learning_rate=0.001, print_every=100, plot_every=100):\n",
        "    model.train()\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        loss = train_epoch(train_dataloader, model, optimizer, criterion, lr_scheduler, use_causal_mask)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if epoch % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) loss: %.4f, lr: %.6f' % (timeSince(start, epoch / n_epochs),\n",
        "                                        epoch, epoch / n_epochs * 100, print_loss_avg, lr_scheduler.get_last_lr()[0]))\n",
        "\n",
        "        if epoch % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "-mTXzFRTpioi",
      "metadata": {
        "id": "-mTXzFRTpioi"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, sentence, input_lang, output_lang):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "\n",
        "        decoder_outputs = model(input_tensor)\n",
        "\n",
        "        _, topi = decoder_outputs.topk(1)\n",
        "        decoded_ids = topi.squeeze()\n",
        "\n",
        "        decoded_words = []\n",
        "        for idx in decoded_ids:\n",
        "            if idx.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            decoded_words.append(output_lang.index2word[idx.item()])\n",
        "    return decoded_words\n",
        "\n",
        "def evaluateRandomly(model, pairs, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words = evaluate(model, pair[0], input_lang, output_lang)\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f405eab9",
      "metadata": {
        "id": "f405eab9"
      },
      "source": [
        "<font size='4' color='red'>Task 2.1: Sanity check of fitting a small training set. (5 points) </font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SQ8fwC876eUp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 829
        },
        "id": "SQ8fwC876eUp",
        "outputId": "9cde9b0b-46b4-47d0-899f-8244a626dd12"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\n",
        "random.seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "def get_num_params(model):\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "\n",
        "# re-set MAX_LENGTH\n",
        "MAX_LENGTH = 8\n",
        "input_lang, output_lang, pairs = prepareData('eng', 'fra', MAX_LENGTH, ENG_PREFIXES, True)\n",
        "np.random.shuffle(pairs)\n",
        "# Let's just use 1000 samples\n",
        "pairs = pairs[:1000]\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "EMBED_DIM = 32\n",
        "NUM_ATTN_HEADS = 4\n",
        "FF_DIM = 96\n",
        "NUM_ENCODER_LAYERS = 4\n",
        "NUM_DECODER_LAYERS = 4\n",
        "###########################################################################\n",
        "# TODO: Define the model to fit to a small training set reasonably well.  #\n",
        "# Your training loss should be around 0.5.                                #\n",
        "###########################################################################\n",
        "model = None\n",
        "###########################################################################\n",
        "#                             END OF YOUR CODE                            #\n",
        "###########################################################################\n",
        "\n",
        "# We will clone the model for later usage\n",
        "import copy\n",
        "model_clone = copy.deepcopy(model)\n",
        "\n",
        "print('Number of parameters for the entire model:\\t{:.3f}M'.format(get_num_params(model) / 1e6))\n",
        "print('Number of parameters for the encoder:\\t{:.3f}M'.format(get_num_params(model.encoder) / 1e6))\n",
        "print('Number of parameters for the decoder:\\t{:.3f}M'.format(get_num_params(model.decoder) / 1e6))\n",
        "\n",
        "train_pairs, train_dataloader, _, _ = get_dataloader(pairs, BATCH_SIZE)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "lr_scheduler = torch.optim.lr_scheduler.ConstantLR(optimizer, factor=1)\n",
        "\n",
        "# Your train loss should be around 0.5.\n",
        "train(train_dataloader, model, 50, optimizer, lr_scheduler, use_causal_mask=True, print_every=5, plot_every=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uex7i9ecptSO",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uex7i9ecptSO",
        "outputId": "9c6351d3-913d-4c7d-8ee8-c66a5a684533"
      },
      "outputs": [],
      "source": [
        "# Let's check the model's output\n",
        "print('\\n=== Sample output on the training set. ===')\n",
        "evaluateRandomly(model, train_pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e85e032",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5e85e032",
        "outputId": "d10f1e3a-8171-4ce8-94d8-59244d39e700"
      },
      "outputs": [],
      "source": [
        "# Let's re-train exactly the same model but without using the causal masks\n",
        "optimizer = optim.Adam(model_clone.parameters(), lr=0.001)\n",
        "lr_scheduler = torch.optim.lr_scheduler.ConstantLR(optimizer, factor=1)\n",
        "\n",
        "train(train_dataloader, model_clone, 50, optimizer, lr_scheduler, use_causal_mask=False, print_every=5, plot_every=5)\n",
        "print('\\n=== Sample output on the training set. ===')\n",
        "evaluateRandomly(model_clone, train_pairs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2d36da4",
      "metadata": {
        "id": "f2d36da4"
      },
      "source": [
        "<font size='4' color='red'>Task 2.2 (inline question): Briefly summarize and explain your observation of the model's performance when it is trained without using the causal masks. (8 points) </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f40c396a",
      "metadata": {
        "id": "f40c396a"
      },
      "source": [
        "[Your answer]: "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-no_x5TskX90",
      "metadata": {
        "id": "-no_x5TskX90"
      },
      "source": [
        "<font size='4' color='red'>Task 2.3: Train a good machine translation model. (20 points) </font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "A_Uq7s02khY3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "A_Uq7s02khY3",
        "outputId": "342d5cb2-78f4-41f5-b256-34ad9f9a4cdc"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\n",
        "random.seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "# re-set MAX_LENGTH\n",
        "MAX_LENGTH = 8\n",
        "\n",
        "ENG_PREFIXES = (\n",
        "    \"i\"\n",
        "    \"he\",\n",
        "    \"she\",\n",
        "    \"you\",\n",
        "    \"we\",\n",
        "    \"they\"\n",
        ")\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData('eng', 'fra', MAX_LENGTH, ENG_PREFIXES, True)\n",
        "np.random.shuffle(pairs)\n",
        "\n",
        "# Due to limited computing budget, we will only train the model for 20 epochs\n",
        "N_EPOCHS = 20\n",
        "\n",
        "###########################################################################\n",
        "# TODO: Tune the hyper parameters. Define the model, optimizer, and       #\n",
        "# learning rate scheduler (lr_scheduler). For your reference, the         #\n",
        "# training loss should be around 0.9. And the testing loss should be      #\n",
        "# smaller than 2.2.                                                       #\n",
        "#                                                                         #\n",
        "# Note that the **maximum** number of parameters for the entire model,    #\n",
        "# encoder, and decoder are 3.5M, 0.8M, and 1.2M, respectively.            #\n",
        "###########################################################################\n",
        "BATCH_SIZE = 32\n",
        "EMBED_DIM = 32\n",
        "NUM_ATTN_HEADS = 4\n",
        "FF_DIM = 96\n",
        "NUM_ENCODER_LAYERS = 4\n",
        "NUM_DECODER_LAYERS = 4\n",
        "LR=0.001\n",
        "\n",
        "model = None\n",
        "\n",
        "# **DO NOT** change this one\n",
        "train_pairs, train_dataloader, test_pairs, test_dataloader = get_dataloader(pairs, BATCH_SIZE)\n",
        "\n",
        "optimizer = None\n",
        "lr_scheduler = None\n",
        "###########################################################################\n",
        "#                             END OF YOUR CODE                            #\n",
        "###########################################################################\n",
        "\n",
        "assert get_num_params(model) / 1e6 < 3.5\n",
        "assert get_num_params(model.encoder) / 1e6 < 0.8\n",
        "assert get_num_params(model.decoder) / 1e6 < 1.2\n",
        "print('Number of parameters for the entire model:\\t{:.3f}M'.format(get_num_params(model) / 1e6))\n",
        "print('Number of parameters for the encoder:\\t{:.3f}M'.format(get_num_params(model.encoder) / 1e6))\n",
        "print('Number of parameters for the decoder:\\t{:.3f}M'.format(get_num_params(model.decoder) / 1e6))\n",
        "\n",
        "# As a reference, your training loss should be around 0.09.\n",
        "train(train_dataloader, model, N_EPOCHS, optimizer, lr_scheduler, learning_rate=LR, use_causal_mask=True, print_every=1, plot_every=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "C_hRaWHmSkh5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_hRaWHmSkh5",
        "outputId": "ed4adfa9-c5bd-4b4c-9e2c-7122551c246d"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def compute_inference_loss(dataloader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for data in tqdm(dataloader):\n",
        "            input_tensor, target_tensor = data\n",
        "\n",
        "            decoder_outputs = model(input_tensor)\n",
        "\n",
        "            loss = F.nll_loss(\n",
        "                decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
        "                target_tensor.view(-1)\n",
        "            )\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "# Your testing loss should be smaller than 2.2\n",
        "test_loss = compute_inference_loss(test_dataloader)\n",
        "print('testing loss: {:.3f}'.format(test_loss))\n",
        "\n",
        "# Let's check the model's output\n",
        "print('\\n=== Sample output on the testing set. ===')\n",
        "evaluateRandomly(model, test_pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "__Yvg8f0wclc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__Yvg8f0wclc",
        "outputId": "34f639a4-ef16-47be-f439-b546e9852ae7"
      },
      "outputs": [],
      "source": [
        "# Let's check the inference-time loss on the training set\n",
        "train_loss = compute_inference_loss(train_dataloader)\n",
        "print('training loss: {:.3f}'.format(train_loss))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1khHw-zjwjuk",
      "metadata": {
        "id": "1khHw-zjwjuk"
      },
      "source": [
        "<font size='4' color='red'>Task 2.4 (inline question): The inference-time loss is higher than the training-time loss for the same training set. Briefly explain why. (7 points) </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8MQ3zP1UwvpG",
      "metadata": {
        "id": "8MQ3zP1UwvpG"
      },
      "source": [
        "[Your answer]:"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
